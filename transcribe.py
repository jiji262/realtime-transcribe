#! python3.7

import sounddevice
import argparse
import os
import numpy as np
import speech_recognition as sr
import whisper
import torch
import threading
import pyaudio
import signal
import time
from faster_whisper import WhisperModel

from datetime import datetime, timedelta
from queue import Queue
from time import sleep
from sys import platform

from PyQt5.QtWidgets import QApplication, QMainWindow, QWidget, QHBoxLayout, QLabel, QTextEdit
from PyQt5.QtGui import QFont, QFontMetrics, QCursor
from PyQt5.QtCore import QChildEvent, Qt, QTimer, QPoint, QRect, QSize, pyqtSignal, QObject

# å…¨å±€å…±äº«å˜é‡ç”¨äºçº¿ç¨‹é—´é€šä¿¡
text_to_display = "Loading captions..."
text_lock = threading.Lock()

def parse_args():
  parser = argparse.ArgumentParser()
  parser.add_argument("--model", default="tiny", help="Model to use",
            choices=["tiny", "base", "small", "medium", "large", "tiny.en", "base.en", "small.en", "medium.en", "large-v3"])
  parser.add_argument("--input", default=None,
            help="Audio input device (can be a device index or partial device name).", type=str)
  parser.add_argument("--input-provider", default="pyaudio",
            choices=["pyaudio", "speech-recognition"],
            help="Default input provider.", type=str)
  parser.add_argument("--no-faster-whisper", action='store_true', default=False,
            help="Disable faster whisper.")

  parser.add_argument("--language", default=None,
            help="Default language.", type=str)
  parser.add_argument("--translate", action='store_true', default=False,
            help="Translate to English.")

  parser.add_argument("--no-fp16", action='store_true', default=False,
            help="Disable fp16.")
  parser.add_argument("--stabilize-turns", default=1,
            help="Turns to stabilize result (before discarding audio record). 0 means never.", type=int)
  parser.add_argument("--min-duration", default=2.0,
            help="Min duration of audio record to keep.", type=float)
  parser.add_argument("--max-duration", default=10.0,
            help="Max duration of audio record to keep.", type=float)
  parser.add_argument("--keep-transcriptions", action='store_true', default=False,
            help="Keep all previous transcriptions")

  parser.add_argument("--font-size", default=30,
            help="Font size of HUD.", type=int)

  # args for input provider 'speech-recognition'
  parser.add_argument("--energy_threshold", default=300,
            help="Energy level for mic to detect.", type=int)
  parser.add_argument("--record_timeout", default=0.3, 
            help="How real time the recording is in seconds.", type=float)
  parser.add_argument("--phrase_timeout", default=0.8, 
            help="How much empty space between recordings before we "
               "consider it a new line in the transcription.", type=float)

  # args for input provider 'pyaudio'
  parser.add_argument("--moving-window", default=10, 
            help="Moving window duration in seconds for transription.", type=int)
  parser.add_argument("--chunk-size", default=1024, 
            help="Audio chunk size for pyaudio.", type=int)
  parser.add_argument("--realtime-mode", action='store_true', default=False,
            help="Enable optimizations for real-time display")
  args = parser.parse_args()
  return args


class HUDText(QTextEdit):
  def __init__(self, font_size):
    super().__init__()

    self.setReadOnly(True)
    self.setAlignment(Qt.AlignLeft|Qt.AlignVCenter)
    self.setStyleSheet("""
      background-color: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                                        stop:0 rgba(20,20,20,0.8), 
                                        stop:1 rgba(40,40,40,0.8));
      color: #FFFFFF;
      border-radius: 15px;
      border: 1px solid rgba(255,255,255,0.3);
      padding: 10px;
      selection-background-color: rgba(30,144,255,0.5);
      selection-color: white;
    """)
    self.setLineWrapMode(QTextEdit.WidgetWidth)
    
    # è®¾ç½®å­—ä½“
    font = QFont("Arial", font_size)
    font.setWeight(QFont.Medium)
    self.setFont(font)
    
    # è®¾ç½®æ–‡æœ¬æ ¼å¼
    self.document().setDefaultStyleSheet("""
      p { margin-bottom: 10px; line-height: 120%; }
      .current { color: #F8F8F8; }
      .previous { color: rgba(255,255,255,0.75); }
    """)
    
    self.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    self.viewport().setCursor(Qt.CursorShape.ArrowCursor)
    
    # è®¾ç½®æ–‡æ¡£è¾¹è·
    document = self.document()
    document.setDocumentMargin(15)

  def mousePressEvent(self, event):
    event.ignore()

  def mouseMoveEvent(self, event):
    event.ignore()

  def mouseReleaseEvent(self, event):
    event.ignore()

class HUD(QMainWindow):
  max_width_percentage = 0.7  # å¢å¤§çª—å£å®½åº¦ï¼Œé¿å…æ–‡æœ¬æ¢è¡Œè¿‡å¤š
  max_height_percentage = 0.35  # å†å¢åŠ é«˜åº¦ä»¥å®¹çº³æ›´å¤šæ–‡æœ¬
  max_lines = 6
  padding = 15
  corner_spacing = 20

  def __init__(self, font_size):
    super().__init__()

    # è®¾ç½®çª—å£å±æ€§
    self.setWindowFlags(Qt.CustomizeWindowHint|Qt.FramelessWindowHint|Qt.WindowStaysOnTopHint|Qt.WindowDoesNotAcceptFocus)
    self.setAttribute(Qt.WA_TranslucentBackground)

    # è®¾ç½®çª—å£é€æ˜åº¦
    self.setWindowOpacity(1.0)

    # è®¾ç½®ä¸­å¤®çª—å£
    central_widget = QWidget()
    layout = QHBoxLayout(central_widget)
    layout.setContentsMargins(10, 10, 10, 10)

    self.text_widget = HUDText(font_size)
    self.text_widget.setParent(central_widget)
    layout.addWidget(self.text_widget)

    self.setCentralWidget(central_widget)

    # é™åˆ¶çª—å£å¤§å°
    screen_geometry = QApplication.desktop().screenGeometry()
    max_width = int(self.max_width_percentage * screen_geometry.width())
    max_height = int(self.max_height_percentage * screen_geometry.height())
    self.setFixedWidth(max_width)
    self.setFixedHeight(max_height)
    
    # å°†çª—å£æ”¾åœ¨å±å¹•åº•éƒ¨ä¸­å¤®
    self.move(int((screen_geometry.width() - max_width) / 2), 
              screen_geometry.height() - max_height - self.corner_spacing)

    # å®šæœŸæ›´æ–°æ–‡æœ¬çš„å®šæ—¶å™¨
    self.update_text_timer = QTimer(self)
    self.update_text_timer.timeout.connect(self.updateTextWidget)
    self.update_text_timer.start(33)  # çº¦30fpsçš„æ›´æ–°é¢‘ç‡

    # æ‹–åŠ¨çª—å£çš„å˜é‡
    self.old_pos = None
    
    # ä¸Šä¸€æ¬¡æ˜¾ç¤ºçš„æ–‡æœ¬
    self.last_displayed_text = ""
    
    # åˆå§‹æµ‹è¯•æ–‡æœ¬
    global text_to_display
    with text_lock:
      text_to_display = "Caption window started\nStart speaking..."
    
    print("HUD window created, initial text set")

  def mousePressEvent(self, event):
    if event.button() == Qt.LeftButton:
      self.old_pos = event.globalPos()

  def mouseMoveEvent(self, event):
    if self.old_pos:
      delta = QPoint(event.globalPos() - self.old_pos)
      self.move(self.x() + delta.x(), self.y() + delta.y())
      self.old_pos = event.globalPos()

  def mouseReleaseEvent(self, event):
    if event.button() == Qt.LeftButton:
      self.old_pos = None

  def updateTextWidget(self):
    # ä»å…¨å±€å˜é‡è·å–å½“å‰æ–‡æœ¬
    global text_to_display
    with text_lock:
      current_text = text_to_display
    
    # è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºå½“å‰æ–‡æœ¬
    if current_text:
      print(f"Current text_to_display: '{current_text[:30]}...' (len={len(current_text)})")
    else:
      print("Current text_to_display is empty")
    
    # åªåœ¨æ–‡æœ¬éç©ºä¸”ä¸ä¸Šæ¬¡ä¸åŒæ—¶æ›´æ–°ï¼Œå¹¶ä¸”ç¡®ä¿æœ€å°é•¿åº¦ï¼Œé¿å…æ˜¾ç¤ºè¿‡çŸ­çš„ç‰‡æ®µ
    if current_text and current_text.strip() and current_text != self.last_displayed_text:
      try:
        # å¤„ç†æ–‡æœ¬ï¼Œç¡®ä¿æ¯å¥è¯æ¢è¡Œæ˜¾ç¤º
        formatted_text = self.format_text(current_text)
        
        # ä½¿ç”¨HTMLæ ¼å¼åŒ–æ–‡æœ¬
        html_text = self.format_text_html(formatted_text)
        
        # æ›´æ–°æ˜¾ç¤ºæ–‡æœ¬
        self.text_widget.setHtml(html_text)
        
        # è®°å½•æ˜¾ç¤ºçš„æ–‡æœ¬
        self.last_displayed_text = current_text
        
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        vertical_scrollbar = self.text_widget.verticalScrollBar()
        vertical_scrollbar.setValue(vertical_scrollbar.maximum())

        # è°ƒè¯•ä¿¡æ¯
        print(f"HUD text updated successfully, length: {len(current_text)}")
        
        # å¼ºåˆ¶é‡ç»˜çª—å£
        self.text_widget.repaint()
      except Exception as e:
        print(f"Error updating caption window: {e}")
        import traceback
        traceback.print_exc()

  def format_text(self, text):
    # å¤„ç†æ–‡æœ¬ï¼Œç¡®ä¿æ¯å¥è¯éƒ½æ¢è¡Œ
    # é¦–å…ˆæŒ‰åŸæœ‰çš„æ¢è¡Œç¬¦åˆ†å‰²
    paragraphs = text.split('\n')
    formatted_paragraphs = []
    
    for paragraph in paragraphs:
      # å¤„ç†æ®µè½å†…çš„å¥å­
      # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰åˆ†å‰²å¥å­ï¼Œä¿ç•™åˆ†éš”ç¬¦
      sentences = []
      current_pos = 0
      for i, char in enumerate(paragraph):
        if char in '.!?ã€‚ï¼ï¼Ÿ':
          if i+1 < len(paragraph) and paragraph[i+1] != ' ':
            # å¦‚æœå¥å·åé¢æ²¡æœ‰ç©ºæ ¼ï¼Œå¯èƒ½æ˜¯ç¼©å†™æˆ–å°æ•°ç‚¹ï¼Œä¸åˆ†å‰²
            continue
          if i+1 < len(paragraph):
            sentences.append(paragraph[current_pos:i+2])
            current_pos = i+2
          else:
            sentences.append(paragraph[current_pos:i+1])
            current_pos = i+1
      
      # æ·»åŠ æœ€åä¸€å¥ï¼ˆå¦‚æœæœ‰ï¼‰
      if current_pos < len(paragraph):
        sentences.append(paragraph[current_pos:])
      
      # å°†å¤„ç†åçš„å¥å­æ·»åŠ åˆ°æ ¼å¼åŒ–æ®µè½
      if sentences:
        formatted_paragraphs.append('\n'.join(sentences))
      else:
        formatted_paragraphs.append(paragraph)
    
    # å°†æ ¼å¼åŒ–çš„æ®µè½ç»„åˆæˆæœ€ç»ˆæ–‡æœ¬
    return '\n\n'.join(formatted_paragraphs)
  
  def format_text_html(self, text):
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºHTMLæ ¼å¼ï¼Œå¢å¼ºå¯è¯»æ€§
    lines = text.split('\n')
    html_parts = ['<html><body>']

    # æ·»åŠ é¡¶éƒ¨è¾¹è·
    html_parts.append('<div style="margin-top:5px;"></div>')

    for i, line in enumerate(lines):
      if line.strip():
        # æœ€åä¸€è¡Œï¼ˆå½“å‰æ­£åœ¨è½¬å½•çš„ï¼‰ä½¿ç”¨è¾ƒäº®çš„é¢œè‰²
        # ä¹‹å‰çš„è¡Œä½¿ç”¨ç¨æš—çš„é¢œè‰²è¡¨ç¤ºå·²å®Œæˆ
        if i == len(lines) - 1:
          # å½“å‰æ­£åœ¨è½¬å½•çš„å¥å­ - ä½¿ç”¨åŠ¨ç”»æ•ˆæœå‡å°‘è·³è·ƒæ„Ÿ
          html_parts.append(f'<p class="current" style="line-height:130%; margin-bottom:8px; letter-spacing:0.5px; color:#F8F8F8; transition: all 0.3s ease-in-out; opacity: 1; transform: translateY(0);">{line}</p>')
        else:
          # å·²å®Œæˆçš„å¥å­ - ç¨æš—ä½†ç¨³å®š
          html_parts.append(f'<p class="previous" style="line-height:130%; margin-bottom:8px; letter-spacing:0.5px; color:rgba(255,255,255,0.85); transition: all 0.3s ease-in-out; opacity: 0.9;">{line}</p>')

    html_parts.append('</body></html>')
    return ''.join(html_parts)

class AudioInputProvider:
  def list_input_devices(self):
    raise NotImplementedError

  def init_input_device(self, device_index):
    raise NotImplementedError

  def start_record(self):
    raise NotImplementedError

  def stop_record(self):
    raise NotImplementedError

  def phrase_cut_off(self, acc_data, new_data):
    raise NotImplementedError

class SpeechRecognitionAudioProvider(AudioInputProvider):
  def __init__(self, args, data_queue, sample_rate):
    self.sample_rate = sample_rate
    self.energy_threshold = args.energy_threshold
    self.record_timeout = args.record_timeout
    self.phrase_timeout = args.phrase_timeout

    self.data_queue = data_queue
    self.stop_listening = None

  def list_input_devices(self):
    try:
      devices = sr.Microphone.list_microphone_names()
      print(f"Detected {len(devices)} microphone devices")
      return devices
    except Exception as e:
      print(f"Error listing microphone devices: {e}")
      return ["Default Microphone"]

  def init_input_device(self, device_index):
    # We use SpeechRecognizer to record our audio because it has a nice feature where it can detect when speech ends.
    try:
      print(f"Initializing microphone, device index: {device_index}")
      self.recorder = sr.Recognizer()
      self.recorder.energy_threshold = self.energy_threshold
      # Definitely do this, dynamic energy compensation lowers the energy threshold dramatically to a point where the SpeechRecognizer never stops recording.
      self.recorder.dynamic_energy_threshold = False
      print(f"Energy threshold set to: {self.energy_threshold}")

      # Try to create the microphone with the given device index
      try:
        print(f"Attempting to initialize microphone with device index {device_index}")
        self.source = sr.Microphone(sample_rate=self.sample_rate, device_index=device_index)
        print(f"Microphone initialized successfully, device index: {device_index}")
      except Exception as e:
        print(f"Failed to initialize microphone with device index {device_index}: {e}")
        print("Attempting to use default microphone")
        self.source = sr.Microphone(sample_rate=self.sample_rate)

      # Test if the microphone works
      try:
        print("Testing microphone functionality...")
        with self.source:
          self.recorder.adjust_for_ambient_noise(self.source)
          print("Microphone test successful, ambient noise adjusted")
      except Exception as e:
        print(f"Error adjusting ambient noise: {e}")
        # Try to create a default microphone instead
        print("Attempting to use alternative method to initialize default microphone")
        self.source = sr.Microphone(sample_rate=self.sample_rate)
        with self.source:
          print("Using default microphone instead")
    
    except Exception as e:
      print(f"Critical SpeechRecognition error: {e}")
      import traceback
      traceback.print_exc()
      raise

  def start_record(self):
    # Create a background thread that will pass us raw audio bytes.
    # We could do this manually but SpeechRecognizer provides a nice helper.
    try:
      print(f"Starting recording, timeout: {self.record_timeout} seconds")
      self.stop_listening = self.recorder.listen_in_background(self.source, self.record_callback, phrase_time_limit=self.record_timeout)
      self.phrase_time = None
      print("Recording started successfully, starting to listen to microphone")
    except Exception as e:
      print(f"Recording start failed: {e}")
      import traceback
      traceback.print_exc()
      raise

  def stop_record(self):
    if self.stop_listening is None:
      print("Warning: Recording not started")
      return

    try:
      self.stop_listening(True)
      print("Recording stopped successfully")
    except Exception as e:
      print(f"Error stopping recording: {e}")

    if hasattr(self, 'phrase_time'):
      del self.phrase_time

  def phrase_cut_off(self, acc_data, new_data):
    now = datetime.now()

    # If enough time has passed between recordings, consider the phrase complete.
    # Clear the current working audio buffer to start over with the new data.
    if self.phrase_time and now - self.phrase_time > timedelta(seconds=self.phrase_timeout):
      # This is the last time we received new audio data from the queue.
      self.phrase_time = now # should set after the transcription is done
      return len(acc_data)
    else:
      return 0

  def record_callback(self, _, audio: sr.AudioData) -> None:
    """
    Threaded callback function to receive audio data when recordings finish.
    audio: An AudioData containing the recorded bytes.
    """
    # Grab the raw bytes and push it into the thread safe queue.
    try:
      data = audio.get_raw_data()
      data_size = len(data)
      if data_size > 0:
        self.data_queue.put(data)
        print(f"Received audio data: {data_size} bytes")
      else:
        print("Warning: Received empty audio data")
      # Print a small indicator that data was received
      print(".", end="", flush=True)
    except Exception as e:
      print(f"Error in recording callback: {e}")

class PyAudioProvider(AudioInputProvider):
  def __init__(self, args, data_queue, sample_rate):
    self.audio = pyaudio.PyAudio()

    self.audio_format = pyaudio.paInt16  # Format of the audio samples
    self.audio_channels = 1  # Number of audio channels (1 for mono, 2 for stereo)
    self.sample_rate = sample_rate  # Sample rate (samples per second)
    self.sample_size = self.audio.get_sample_size(self.audio_format)
    self.audio_chunk = args.chunk_size

    self.moving_window = args.moving_window

    self.device_index = None
    self.stop_event = threading.Event()
    self.stream = None  # Add stream reference for proper cleanup

    self.data_queue = data_queue
    print(f"PyAudio initialized successfully, sample rate: {sample_rate}Hz, chunk size: {args.chunk_size}")

  def __del__(self):
    """Ensure PyAudio is properly terminated when object is destroyed"""
    try:
      if hasattr(self, 'audio') and self.audio:
        # Only terminate if not already terminated
        if hasattr(self.audio, '_pa'):
          self.audio.terminate()
          self.audio = None
    except Exception as e:
      print(f"Error terminating PyAudio: {e}")

  def list_input_devices(self):
    devices = []
    try:
      print("Finding available audio input devices...")
      for idx in range(self.audio.get_device_count()):
        device_info = self.audio.get_device_info_by_index(idx)
        # Only include devices that support input
        if device_info.get('maxInputChannels', 0) > 0:
          devices.append(device_info['name'])
          print(f"Found input device {idx}: {device_info['name']}")
      
      # If no input devices found, add a default option
      if not devices:
        print("No input devices found, using default device")
        devices.append("Default Input Device")
    except Exception as e:
      print(f"Error listing audio devices: {e}")
      devices.append("Default Input Device")
    
    return devices

  def init_input_device(self, device_index):
    # Validate the device supports input
    try:
      print(f"Initializing audio device, index: {device_index}")
      device_info = self.audio.get_device_info_by_index(device_index)
      if device_info.get('maxInputChannels', 0) <= 0:
        print(f"Warning: Device {device_index} does not support input, attempting to use default input device")
        device_index = self.audio.get_default_input_device_info()['index']
    except Exception as e:
      print(f"Error with device {device_index}: Using default device: {e}")
      device_index = None
    
    self.device_index = device_index
    print(f"Using input device index: {self.device_index}")

  def start_record(self):
    print("Starting PyAudio recording thread...")
    self.stop_event.clear()

    self.record_thread = threading.Thread(target=self._record_audio)
    self.record_thread.daemon = True  # Make thread daemon so it exits when main program exits
    self.record_thread.start()
    print("PyAudio recording thread started")

  def stop_record(self):
    try:
      print("Stopping PyAudio recording...")
      self.stop_event.set()

      # Close stream if it exists
      if hasattr(self, 'stream') and self.stream:
        try:
          if self.stream.is_active():
            self.stream.stop_stream()
          self.stream.close()
          self.stream = None
        except Exception as e:
          print(f"Error closing audio stream: {e}")

      if hasattr(self, 'record_thread') and self.record_thread.is_alive():
        self.record_thread.join(timeout=2.0)  # Wait up to 2 seconds for thread to finish
        print("PyAudio recording stopped")

      # Terminate PyAudio
      if hasattr(self, 'audio') and self.audio:
        try:
          self.audio.terminate()
          self.audio = None
        except Exception as e:
          print(f"Error terminating PyAudio: {e}")

    except Exception as e:
      print(f"Error stopping recording: {e}")

  def phrase_cut_off(self, acc_data, new_data):
    if (exceed := len(acc_data) + len(new_data) - self.sample_rate*self.moving_window) > 0:
      return exceed
    else:
      return 0

  def _record_audio(self):
    def stream_callback(in_data, frame_count, time_info, status):
      # Add small visual feedback that we're receiving audio
      data_size = len(in_data)
      if data_size > 0:
        # éŸ³é¢‘æ•°æ®æœ‰æ•ˆï¼Œæ”¾å…¥é˜Ÿåˆ—
        self.data_queue.put(in_data)
        # è§†è§‰åé¦ˆï¼Œä½†é™åˆ¶æ‰“å°é¢‘ç‡
        print(".", end="", flush=True)
      else:
        print("x", end="", flush=True)  # æ”¶åˆ°ç©ºæ•°æ®æ—¶æ˜¾ç¤ºx
      
      # è¿”å›Noneè¡¨ç¤ºæ— è¾“å‡ºæ•°æ®ï¼Œpyaudio.paContinueè¡¨ç¤ºç»§ç»­å½•åˆ¶
      return (None, pyaudio.paContinue)

    try:
      # å°è¯•å¤šæ¬¡åˆå§‹åŒ–éŸ³é¢‘æµ
      max_attempts = 3
      attempts = 0
      stream = None
      
      while attempts < max_attempts and not self.stop_event.is_set():
        attempts += 1
        try:
          # Open audio stream with the selected input device
          if self.device_index is None:
            # Use default input device if none specified
            print(f"Attempt {attempts}/{max_attempts}: Using default input device")
            stream = self.audio.open(
              format=self.audio_format,
              channels=self.audio_channels,
              rate=self.sample_rate,
              input=True,
              frames_per_buffer=self.audio_chunk,
              stream_callback=stream_callback,
            )
          else:
            # Use specified input device
            print(f"Attempt {attempts}/{max_attempts}: Using device index {self.device_index}")
            stream = self.audio.open(
              format=self.audio_format,
              channels=self.audio_channels,
              rate=self.sample_rate,
              input=True,
              input_device_index=self.device_index,
              frames_per_buffer=self.audio_chunk,
              stream_callback=stream_callback,
            )
          
          # æˆåŠŸæ‰“å¼€æµ
          print(f"Audio stream started, device index: {self.device_index}")
          break
        except Exception as e:
          print(f"Error opening audio stream (attempt {attempts}/{max_attempts}): {e}")
          if attempts < max_attempts:
            print("Retrying in 1 second...")
            time.sleep(1)
            if self.device_index is not None and attempts == max_attempts - 1:
              print("Trying with default device as last resort")
              self.device_index = None
          else:
            raise
      
      if not stream or not stream.is_active():
        print("CRITICAL ERROR: Failed to open an active audio stream")
        return
      
      # ç›´æ¥æµ‹è¯•éŸ³é¢‘æ•è·
      print("Testing audio capture for 2 seconds...")
      test_start = time.time()
      while time.time() - test_start < 2.0 and not self.stop_event.is_set():
        if self.data_queue.qsize() > 0:
          print(f"\nAudio capture test successful! Queue size: {self.data_queue.qsize()}")
          break
        sleep(0.1)
      
      if self.data_queue.qsize() == 0:
        print("\nWARNING: No audio data received during test. Check your microphone.")
      
      # Store stream reference for cleanup
      self.stream = stream

      # Keep the stream active until stop is requested
      while not self.stop_event.is_set() and stream.is_active():
        # æ¯éš”ä¸€æ®µæ—¶é—´æŠ¥å‘Šä¸€ä¸‹é˜Ÿåˆ—å¤§å°
        if self.data_queue.qsize() > 0 and self.data_queue.qsize() % 20 == 0:
          print(f"\nAudio queue size: {self.data_queue.qsize()}")
        sleep(0.1)

      # Close the audio stream
      if stream:
        try:
          stream.stop_stream()
          stream.close()
          self.stream = None
          print("Audio stream closed")
        except Exception as e:
          print(f"Error closing stream: {e}")
      
    except Exception as e:
      print(f"Critical error in audio recording: {e}")
      import traceback
      traceback.print_exc()
      # No automatic retry with default device, let the error bubble up

class Transcriber():
  n_context = 5
  max_transcription_history = 100

  def __init__(self, args):
    self.args = args
    self.language = args.language
    self.sample_rate = whisper.audio.SAMPLE_RATE
    # Use CPU by default for more compatibility, allow opt-in to GPU
    self.compute_device = "cpu"
    if torch.cuda.is_available():
      print("CUDA is available! To use GPU, restart the program without --no-faster-whisper flag")
      
    self.model_name = args.model

    # Thread safe Queue for passing data from the threaded recording callback.
    self.data_queue = Queue()
    self.transcribe_thread = None
    self.stop_event = threading.Event()

    if args.input_provider == "speech-recognition":
      self.input_provider = SpeechRecognitionAudioProvider(args=self.args, data_queue=self.data_queue, sample_rate=self.sample_rate)
    elif args.input_provider == "pyaudio":
      self.input_provider = PyAudioProvider(args=self.args, data_queue=self.data_queue, sample_rate=self.sample_rate)

    print(f"Using {args.input_provider} as input provider")
    print(f"Using {self.model_name} model")
    print(f"Computing on {self.compute_device}")

    self.init_input_device(args)

    print(f"Loading model {self.model_name}...")
    # Load / Download model
    start_time = time.time()
    try:
      if self.args.no_faster_whisper:
        self.audio_model = whisper.load_model(self.model_name, device=self.compute_device)
      else:
        self.audio_model = WhisperModel(self.model_name, device=self.compute_device)
      print(f"Model loaded in {time.time() - start_time:.2f} seconds")
    except Exception as e:
      print(f"Error loading model: {e}")
      raise

    # Cue the user that we're ready to go.
    print("System ready. Starting transcription...\n")

  def init_input_device(self, args):
    device_index = None

    if args.input is not None:
      try:
        # æ£€æŸ¥æ˜¯å¦è¾“å…¥çš„æ˜¯æ•°å­—ç´¢å¼•
        if args.input.isdigit():
          device_index = int(args.input)
          devices = self.input_provider.list_input_devices()
          if device_index < 0 or device_index >= len(devices):
            print(f"Warning: Device index {device_index} out of range, will list available devices")
            device_index = None
          else:
            print(f"Using specified device index: {device_index} ({devices[device_index]})")
        else:
          # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œå°è¯•åŒ¹é…è®¾å¤‡åç§°
          for idx, name in enumerate(self.input_provider.list_input_devices()):
            if args.input.lower() in name.lower():  # ä½¿ç”¨éƒ¨åˆ†åŒ¹é…è€Œä¸æ˜¯ç²¾ç¡®åŒ¹é…
              device_index = idx
              print(f"Found matching device: {idx}. {name}")
              break
      except Exception as e:
        print(f"Error finding specified input device: {e}")
        device_index = None

    if device_index is None:
      try:
        # æ‰“å°å¯ç”¨éŸ³é¢‘è®¾å¤‡åˆ—è¡¨
        print("Available input devices:")
        devices = list(self.input_provider.list_input_devices())
        for idx, name in enumerate(devices):
          print(f"{idx}. {name}")

        if args.input is None:
          # é€‰æ‹©æ‰€éœ€è¾“å…¥è®¾å¤‡çš„ç´¢å¼•
          default_device = 0
          # ä¼˜å…ˆæŸ¥æ‰¾MacBook Proéº¦å…‹é£ï¼Œç„¶åæ˜¯å…¶ä»–éº¦å…‹é£
          for idx, name in enumerate(devices):
            if "MacBook Pro" in name or "éº¦å…‹é£" in name:
              default_device = idx
              break
          # å¦‚æœæ²¡æ‰¾åˆ°MacBook Proéº¦å…‹é£ï¼ŒæŸ¥æ‰¾å…¶ä»–éº¦å…‹é£
          if default_device == 0:
            for idx, name in enumerate(devices):
              if "mic" in name.lower() or "microphone" in name.lower():
                default_device = idx
                break
          
          print(f"Default device will be {default_device}: {devices[default_device]} in 5 seconds...")
          print("Enter device number to override (or press Enter to use default):", end="", flush=True)
          
          # è®¾ç½®è¶…æ—¶è¯»å–è¾“å…¥
          import select
          import sys
          
          # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·è¾“å…¥ï¼Œæœ€å¤šç­‰å¾…5ç§’
          ready, _, _ = select.select([sys.stdin], [], [], 5)
          if ready:
            user_input = sys.stdin.readline().strip()
            if user_input.isdigit() and 0 <= int(user_input) < len(devices):
              device_index = int(user_input)
              print(f"User selected device: {device_index}. {devices[device_index]}")
            else:
              if user_input:
                print(f"Invalid input '{user_input}', using default device")
              device_index = default_device
          else:
            print("\nNo input received, using default device")
            device_index = default_device
        else:
          print(f"Could not find device matching '{args.input}', please select from available devices")
          return self.init_input_device(argparse.Namespace(**{**vars(args), 'input': None}))  # é‡æ–°è°ƒç”¨ä½†æ¸…é™¤inputå‚æ•°
      except Exception as e:
        print(f"Error in device selection: {e}")
        device_index = None  # ä½¿ç”¨é»˜è®¤è®¾å¤‡

    try:
      self.input_provider.init_input_device(device_index)
    except Exception as e:
      print(f"Critical error initializing input device: {e}")
      raise

  def start_transcribe_thread(self):
    if self.transcribe_thread is not None:
      print("Warning: Transcription thread already running")
      return

    self.transcribe_thread = threading.Thread(target=self.listen)
    self.transcribe_thread.daemon = True
    self.transcribe_thread.start()
    print("Transcription thread started")

  def stop_transcribe_thread(self):
    if self.transcribe_thread is None:
      return

    print("Stopping transcription thread...")
    self.stop_event.set()
    self.transcribe_thread.join(timeout=5.0)  # Wait up to 5 seconds
    print("Transcription thread stopped")

  def update_hud_text(self, text):
    # æ›´æ–°å…¨å±€æ–‡æœ¬å˜é‡
    global text_to_display
    with text_lock:
      # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
      if text is None:
        print("Warning: Attempted to update with None text, ignored")
        return

      if not isinstance(text, str):
        text = str(text)
        print(f"Warning: Non-string text converted to: {text}")

      # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦ä½†ä¿ç•™åŸºæœ¬æ ¼å¼
      cleaned_text = ' '.join([line.strip() for line in text.split('\n')])

      # ç¡®ä¿æ–‡æœ¬éç©º
      if cleaned_text.strip():
        # æ›´æ–°å…¨å±€å˜é‡
        text_to_display = cleaned_text
        # é™åˆ¶æ—¥å¿—é•¿åº¦ä»¥é¿å…åˆ·å±
        preview = cleaned_text[:50] + "..." if len(cleaned_text) > 50 else cleaned_text
        print(f"Updated global text_to_display: '{preview}' (len={len(cleaned_text)})")

        # Note: UI updates are handled by the main thread timer, no need to force update here
      else:
        print("Warning: Attempted to update with empty text, ignored")

  def is_sentence_complete(self, text):
    """æ£€æµ‹å¥å­æ˜¯å¦å®Œæ•´ï¼ˆä»¥å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰ç»“å°¾ï¼‰"""
    if not text or not text.strip():
      return False

    text = text.strip()
    # æ£€æŸ¥æ˜¯å¦ä»¥å¥å­ç»“æŸç¬¦ç»“å°¾
    sentence_endings = ['.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ', '...', ':', 'ï¼š']
    return any(text.endswith(ending) for ending in sentence_endings)

  def manage_caption_history(self, new_texts, caption_history, current_caption, last_complete_caption, max_lines=5):
    """
    æ™ºèƒ½ç®¡ç†å­—å¹•å†å²ï¼Œé˜²æ­¢è·³è·ƒï¼Œæ”¯æŒå¤šè¡Œæ»šåŠ¨æ˜¾ç¤º
    è¿”å›: (updated_history, updated_current, updated_last_complete, display_text)
    """
    if not new_texts:
      return caption_history, current_caption, last_complete_caption, None

    # è·å–æœ€æ–°çš„è½¬å½•æ–‡æœ¬
    latest_text = new_texts[-1] if new_texts else ""
    latest_text = latest_text.strip()

    if not latest_text:
      return caption_history, current_caption, last_complete_caption, None

    # æ€»æ˜¯æ·»åŠ æ–°çš„è½¬å½•ç»“æœåˆ°å†å²ä¸­ï¼ˆä¸ç®¡æ˜¯å¦å®Œæ•´ï¼‰
    if latest_text and latest_text != last_complete_caption:
      # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æ–‡æœ¬
      if not caption_history or caption_history[-1] != latest_text:
        caption_history.append(latest_text)
        last_complete_caption = latest_text
        print(f"Added new caption: {latest_text}")

        # ä¿æŒæœ€å¤§è¡Œæ•°é™åˆ¶ï¼Œå®ç°å‘ä¸Šæ»šåŠ¨æ•ˆæœ
        if len(caption_history) > max_lines:
          caption_history = caption_history[-max_lines:]
          print(f"Caption history trimmed to {max_lines} lines")

    # æ„å»ºæ˜¾ç¤ºæ–‡æœ¬ - æ˜¾ç¤ºæ‰€æœ‰å†å²è®°å½•
    display_lines = []
    for sentence in caption_history:
      if sentence.strip():
        display_lines.append(sentence.strip())

    # ç”Ÿæˆæœ€ç»ˆæ˜¾ç¤ºæ–‡æœ¬
    display_text = '\n'.join(display_lines) if display_lines else None

    return caption_history, current_caption, last_complete_caption, display_text







  def listen(self):
    args = self.args
    transcription = []
    last_texts = []
    texts = []  # Initialize texts variable to prevent UnboundLocalError
    result = {'segments': []}  # Initialize result variable to prevent UnboundLocalError
    realtime_mode = args.realtime_mode
    
    # æ·»åŠ ç¨³å®šæ˜¾ç¤ºç›¸å…³å˜é‡
    stable_text = ""
    text_stability_count = 0
    min_stability_count = 2  # è‡³å°‘éœ€è¦å¤šå°‘æ¬¡ç›¸åŒç»“æœæ‰æ›´æ–°æ˜¾ç¤º
    last_shown_texts = []
    last_displayed_text_length = 0  # è·Ÿè¸ªä¸Šæ¬¡æ˜¾ç¤ºçš„æ–‡æœ¬é•¿åº¦
    min_text_change = 3  # æœ€å°æ–‡æœ¬å˜åŒ–é‡ï¼Œå°äºæ­¤å€¼ä¸æ›´æ–°æ˜¾ç¤º

    # å­—å¹•å†å²ç®¡ç† - é˜²æ­¢è·³è·ƒ
    caption_history = []  # å·²å®Œæˆçš„å­—å¹•å¥å­
    current_caption = ""  # å½“å‰æ­£åœ¨è½¬å½•çš„å¥å­
    max_caption_lines = 5  # æœ€å¤šæ˜¾ç¤ºçš„å­—å¹•è¡Œæ•°
    last_complete_caption = ""  # ä¸Šæ¬¡çš„å®Œæ•´å­—å¹•ï¼Œç”¨äºæ£€æµ‹æ–°å¥å­

    # å­—å¹•æ˜¾ç¤ºç¨³å®šæ€§æ§åˆ¶
    last_transcription_result = ""  # ä¸Šæ¬¡çš„è½¬å½•ç»“æœ
    last_result_display_time = 0  # ä¸Šæ¬¡æ˜¾ç¤ºç»“æœçš„æ—¶é—´
    result_display_duration = 5.0  # è½¬å½•ç»“æœæ˜¾ç¤ºæŒç»­æ—¶é—´ï¼ˆç§’ï¼‰- å¢åŠ åˆ°5ç§’
    is_showing_result = False  # æ˜¯å¦æ­£åœ¨æ˜¾ç¤ºè½¬å½•ç»“æœ
    min_silence_before_new_transcription = 2.0  # é™éŸ³å¤šé•¿æ—¶é—´åæ‰å¼€å§‹æ–°çš„è½¬å½•





    try:
      print("Starting recording...")
      self.input_provider.start_record()
      print("Recording started, waiting for audio data")

      if args.no_faster_whisper:
        import torch
        acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
        last_transcription_time = time.time()
        last_update_time = time.time()
        last_audio_debug_time = time.time()
        empty_queue_count = 0
        
        # ç¡®ä¿å¯åŠ¨æ—¶æ›´æ–°UI
        self.update_hud_text("ğŸ¤ æ­£åœ¨ç›‘å¬æ‚¨çš„è¯­éŸ³...\nè¯·æ¸…æ™°åœ°è¯´è¯")
      else:
        acc_audio_data = np.zeros((0,), dtype=np.float32)
        
        last_transcription_time = time.time()
        last_update_time = time.time()
        last_audio_debug_time = time.time()
        empty_queue_count = 0
        
        # ç¡®ä¿å¯åŠ¨æ—¶æ›´æ–°UI
        self.update_hud_text("ğŸ¤ æ­£åœ¨ç›‘å¬æ‚¨çš„è¯­éŸ³...\nè¯·æ¸…æ™°åœ°è¯´è¯")
        
      while not self.stop_event.is_set():
        try:
          current_time = time.time()
          
          # æ¯10ç§’æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
          if current_time - last_audio_debug_time > 10:
            print(f"Transcription status: Cumulative audio length {len(acc_audio_data)/self.sample_rate:.2f} seconds")
            last_audio_debug_time = current_time
          
          # è·å–éŸ³é¢‘æ•°æ®
          try:
            # å®‰å…¨åœ°è·å–é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®
            audio_data_list = []
            while not self.data_queue.empty():
              try:
                data = self.data_queue.get_nowait()
                audio_data_list.append(data)
              except:
                break

            audio_data = b''.join(audio_data_list)

            if len(audio_data_list) > 0:
              print(f"Detected {len(audio_data_list)} audio data packets")

            if len(audio_data) > 0:
              print(f"Received audio data: {len(audio_data)} bytes")
              empty_queue_count = 0
            else:
              empty_queue_count += 1
              if empty_queue_count % 100 == 0:
                print(f"Warning: Continued {empty_queue_count} times without receiving audio data")
                # Don't reinitialize automatically as it can cause crashes
                # if empty_queue_count >= 200:
                #   print("Attempting to reinitialize microphone...")
                #   self.input_provider.stop_record()
                #   time.sleep(1)
                #   self.input_provider.start_record()
                #   empty_queue_count = 0
          except Exception as e:
            print(f"Error getting audio data: {e}")
            sleep(0.05)
            continue

          # å³ä½¿æ²¡æœ‰æ–°éŸ³é¢‘æ•°æ®ï¼Œä¹Ÿè¦å®šæœŸå°è¯•è½¬å½•å½“å‰ç´¯ç§¯çš„éŸ³é¢‘
          if len(audio_data) == 0:
            # å¦‚æœå·²ç»æœ‰ä¸€å®šé‡çš„éŸ³é¢‘æ•°æ®ä¸”ç»è¿‡äº†è¶³å¤Ÿçš„æ—¶é—´
            if len(acc_audio_data) > 0 and current_time - last_transcription_time > 1.0:
              print(f"No new data, but processing existing {len(acc_audio_data)/self.sample_rate:.2f} seconds of audio")
              # ä¸è¦continueï¼Œè®©ç¨‹åºç»§ç»­å¤„ç†ç°æœ‰æ•°æ®
            else:
              sleep(0.05)
              continue

          # è½¬æ¢éŸ³é¢‘æ ¼å¼
          try:
            # Convert in-ram buffer to something the model can use directly without needing a temp file.
            # Convert data from 16 bit wide integers to floating point with a width of 32 bits.
            # Clamp the audio stream frequency to a PCM wavelength compatible default of 32768hz max.
            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            if args.no_faster_whisper:
              audio_torch = torch.from_numpy(audio_np).to(device=self.compute_device)
              acc_audio_data = torch.hstack([acc_audio_data, audio_torch])
            else:
              acc_audio_data = np.hstack([acc_audio_data, audio_np])

            print(f"Audio data processed, current cumulative {len(acc_audio_data)/self.sample_rate:.2f} seconds")
          except Exception as e:
            print(f"Error processing audio data: {e}")
            continue

          # Apply phrase cut off after accumulating data
          if args.stabilize_turns <= 0:
            phrase_cut_off = self.input_provider.phrase_cut_off(acc_audio_data, audio_data)
            if phrase_cut_off > 0:
              acc_audio_data = acc_audio_data[phrase_cut_off:]
              print(f"Applied phrase cut off: {phrase_cut_off} samples, remaining: {len(acc_audio_data)/self.sample_rate:.2f} seconds")

          # åœ¨å®æ—¶æ¨¡å¼ä¸‹ï¼Œå‡å°æ‰€éœ€çš„æœ€å°éŸ³é¢‘æ•°æ®é‡
          min_audio_length = 0.5 if realtime_mode else 1.0  # ç§’

          # æ£€æŸ¥æ˜¯å¦æ­£åœ¨æ˜¾ç¤ºè½¬å½•ç»“æœ
          if is_showing_result and current_time - last_result_display_time < result_display_duration:
            # æ­£åœ¨æ˜¾ç¤ºè½¬å½•ç»“æœï¼Œæš‚åœè½¬å½•
            sleep(0.2)  # å¢åŠ ç¡çœ æ—¶é—´ï¼Œå‡å°‘CPUä½¿ç”¨
            continue
          elif is_showing_result and current_time - last_result_display_time >= result_display_duration:
            # è½¬å½•ç»“æœæ˜¾ç¤ºæ—¶é—´å·²åˆ°ï¼Œä½†ä¸ç«‹å³æ¢å¤ç›‘å¬çŠ¶æ€
            # ä¿æŒå½“å‰å­—å¹•æ˜¾ç¤ºï¼Œåªæ˜¯å…è®¸æ–°çš„è½¬å½•
            is_showing_result = False
            print("Result display time expired, allowing new transcription...")
            # ä¸ç«‹å³æ›´æ”¹æ˜¾ç¤ºæ–‡æœ¬ï¼Œä¿æŒå­—å¹•ç¨³å®š

          # æ£€æŸ¥æ˜¯å¦åº”è¯¥è¿›è¡Œè½¬å½•
          should_transcribe = False

          # å¦‚æœéŸ³é¢‘æ•°æ®è¶³å¤Ÿé•¿ï¼Œç«‹å³è½¬å½•
          if len(acc_audio_data) >= self.sample_rate * min_audio_length:
            print(f"Audio data sufficient ({len(acc_audio_data)/self.sample_rate:.2f} seconds), starting transcription...")
            should_transcribe = True
          # å¦‚æœéŸ³é¢‘æ•°æ®ä¸å¤Ÿé•¿ï¼Œä½†å·²ç»ç­‰å¾…äº†è¶³å¤Ÿé•¿çš„æ—¶é—´ï¼Œä¹Ÿè¿›è¡Œè½¬å½•
          elif current_time - last_transcription_time >= 3.0 and len(acc_audio_data) > 0:
            print(f"Timeout reached, processing {len(acc_audio_data)/self.sample_rate:.2f} seconds of audio")
            should_transcribe = True
          else:
            print(f"Audio data too short ({len(acc_audio_data)/self.sample_rate:.2f} seconds), waiting for more data...")
            continue

          # åªæœ‰å½“åº”è¯¥è½¬å½•æ—¶æ‰ç»§ç»­
          if not should_transcribe:
            continue

          # å¼ºåˆ¶è¿›è¡Œè½¬å½•ï¼Œå³ä½¿éŸ³é¢‘æ•°æ®è¾ƒå°‘
          if len(acc_audio_data)/self.sample_rate >= 1.0:  # å¦‚æœæœ‰è‡³å°‘1ç§’çš„éŸ³é¢‘
            print(f"Forcing transcription with {len(acc_audio_data)/self.sample_rate:.2f} seconds of audio")
            # ä¸è¦breakï¼Œç»§ç»­æ‰§è¡Œè½¬å½•é€»è¾‘

          # æ›´æ–°æœ€åè½¬å½•æ—¶é—´
          last_transcription_time = current_time

          # è¿›è¡Œè½¬å½•
          try:
            print(f"Starting transcription of {len(acc_audio_data)/self.sample_rate:.2f} seconds of audio...")

            # ç¡®ä¿éŸ³é¢‘æ•°æ®ä¸ä¸ºç©º
            if len(acc_audio_data) == 0:
              print("Warning: No audio data to transcribe")
              continue

            # ç®€åŒ–çš„éŸ³é¢‘æ£€æŸ¥
            if isinstance(acc_audio_data, np.ndarray):
              audio_np = acc_audio_data.astype(np.float32)
            else:
              audio_np = np.array(acc_audio_data, dtype=np.float32)

            audio_max = np.max(np.abs(audio_np))
            print(f"Audio max amplitude: {audio_max:.6f}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯é™éŸ³
            if audio_max < 0.001:  # æé«˜é˜ˆå€¼
              print("Audio appears to be silent, skipping transcription")
              acc_audio_data = np.array([], dtype=np.float32)
              continue

            print("Proceeding with transcription...")

            # åªæœ‰åœ¨æ²¡æœ‰å­—å¹•å†å²æ—¶æ‰æ˜¾ç¤ºæ­£åœ¨è½¬å½•çš„æç¤º
            if not caption_history:
              self.update_hud_text("ğŸ¤ æ­£åœ¨è½¬å½•æ‚¨çš„è¯­éŸ³...")
            else:
              print("Skipping transcription indicator - preserving caption history")

            # ç®€åŒ–çš„å‚æ•°è®¾ç½®
            params = {
              'beam_size': 1,
              'best_of': 1,
              'temperature': [0.0],
              'no_speech_threshold': 0.3,
              'condition_on_previous_text': False
            }

            # è¿›è¡Œè½¬å½•
            try:
              print("Calling audio_model.transcribe...")

              # æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†éŸ³é¢‘æ•°æ®
              if args.no_faster_whisper:
                # æ ‡å‡†whisper - éœ€è¦å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorchå¼ é‡
                if isinstance(acc_audio_data, np.ndarray):
                  audio_tensor = torch.from_numpy(acc_audio_data.astype(np.float32))
                  result = self.audio_model.transcribe(audio_tensor, language=self.language)
                else:
                  result = self.audio_model.transcribe(acc_audio_data, language=self.language)
              else:
                # faster_whisper - ç›´æ¥ä½¿ç”¨numpyæ•°ç»„
                result = self.audio_model.transcribe(
                  acc_audio_data,
                  language=self.language,
                  **params,
                )

              print("Transcription call completed successfully")
            except Exception as transcribe_error:
              print(f"Error during transcription call: {transcribe_error}")
              import traceback
              traceback.print_exc()
              if args.no_faster_whisper:
                acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
              else:
                acc_audio_data = np.array([], dtype=np.float32)
              continue

            print("Transcription completed, processing result...")

            # æå–è½¬å½•æ–‡æœ¬
            texts = []
            try:
              if not args.no_faster_whisper:
                # faster_whisperç»“æœå¤„ç†
                print("Processing faster_whisper result...")
                segments, info = result
                print(f"Got segments and info: {info}")

                # å°è¯•å¤šç§æ–¹æ³•å®‰å…¨åœ°æå–è½¬å½•æ–‡æœ¬
                print("Attempting to safely extract transcription text...")
                texts = []

                # æ£€æŸ¥æ˜¯å¦æœ‰è¯­éŸ³æ´»åŠ¨
                if info.duration_after_vad > 0:
                  print(f"Voice activity detected: {info.duration_after_vad} seconds")

                  # æ–¹æ³•1: å°è¯•æ£€æŸ¥segmentsçš„ç±»å‹å’Œå±æ€§
                  try:
                    print(f"Segments type: {type(segments)}")
                    print(f"Segments has __iter__: {hasattr(segments, '__iter__')}")
                    print(f"Segments has __len__: {hasattr(segments, '__len__')}")

                    # å°è¯•è·å–segmentsçš„é•¿åº¦ï¼ˆå¦‚æœæ”¯æŒï¼‰
                    try:
                      seg_len = len(segments)
                      print(f"Segments length: {seg_len}")
                    except:
                      print("Segments does not support len()")

                    # æ–¹æ³•2: å®Œå…¨è·³è¿‡segmentsè¿­ä»£ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ³•
                    print("Skipping segments iteration, using alternative approach...")

                    # å°è¯•é‡æ–°è½¬å½•ï¼Œä½†ä½¿ç”¨ä¸åŒçš„å‚æ•°æ¥è·å–ç®€å•ç»“æœ
                    try:
                      print("Attempting re-transcription with simpler parameters...")

                      # ä½¿ç”¨æ›´ç®€å•çš„å‚æ•°é‡æ–°è½¬å½•åŒä¸€æ®µéŸ³é¢‘
                      simple_segments, simple_info = self.audio_model.transcribe(
                        acc_audio_data,
                        language=self.language,
                        beam_size=1,
                        word_timestamps=False,
                        condition_on_previous_text=False,
                        vad_filter=False  # å…³é—­VADè¿‡æ»¤
                      )

                      print("Simple transcription completed, attempting to get text...")

                      # å°è¯•ä»ç®€å•è½¬å½•ä¸­è·å–æ–‡æœ¬
                      simple_text_parts = []
                      segment_count = 0

                      for segment in simple_segments:
                        if segment_count >= 1:  # åªå–ç¬¬ä¸€ä¸ªsegment
                          break
                        if hasattr(segment, 'text') and segment.text:
                          simple_text_parts.append(segment.text.strip())
                          print(f"Got simple text: '{segment.text.strip()}'")
                        segment_count += 1

                      if simple_text_parts:
                        combined_text = ' '.join(simple_text_parts)
                        texts.append(combined_text)
                        print(f"Successfully got text via re-transcription: '{combined_text}'")
                      else:
                        print("Re-transcription also failed to get text")
                        texts = [f"âœ“ æ£€æµ‹åˆ° {info.duration_after_vad:.2f}ç§’ è¯­éŸ³"]

                    except Exception as retranscribe_error:
                      print(f"Re-transcription failed: {retranscribe_error}")
                      texts = [f"âœ“ æ£€æµ‹åˆ° {info.duration_after_vad:.2f}ç§’ è¯­éŸ³"]

                  except Exception as segments_error:
                    print(f"Error analyzing segments: {segments_error}")
                    texts = [f"âœ“ æ£€æµ‹åˆ° {info.duration_after_vad:.2f}ç§’ è¯­éŸ³"]
                else:
                  print("No voice activity detected")
                  texts = []
              else:
                # æ ‡å‡†whisperç»“æœå¤„ç†
                print("Processing standard whisper result...")
                try:
                  print(f"Standard whisper result: {result}")

                  if 'text' in result and result['text'].strip():
                    texts.append(result['text'].strip())
                    print(f"Got text from standard whisper: '{result['text'].strip()}'")
                  else:
                    print("Standard whisper returned no text")
                    texts = []
                except Exception as std_whisper_error:
                  print(f"Error with standard whisper: {std_whisper_error}")
                  texts = []
            except Exception as process_error:
              print(f"Error processing transcription result: {process_error}")
              import traceback
              traceback.print_exc()
              texts = []

            if texts:
              print(f"Transcription result: {texts}")

              # ä½¿ç”¨å­—å¹•å†å²ç®¡ç†åŠŸèƒ½
              caption_history, current_caption, last_complete_caption, display_text = self.manage_caption_history(
                texts, caption_history, current_caption, last_complete_caption, max_caption_lines
              )

              if display_text:
                print(f"Updating HUD with managed text: {display_text}")
                self.update_hud_text(display_text)

                # è®¾ç½®ç»“æœæ˜¾ç¤ºçŠ¶æ€ - å»¶é•¿æ˜¾ç¤ºæ—¶é—´
                last_transcription_result = display_text
                last_result_display_time = current_time
                is_showing_result = True
                print(f"Showing result for {result_display_duration} seconds")
              else:
                print("No display text generated from caption management")

              # è½¬å½•æˆåŠŸåï¼Œæ¸…ç©ºéŸ³é¢‘ç¼“å†²åŒº
              print("Clearing audio buffer after successful transcription")
              if args.no_faster_whisper:
                acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
              else:
                acc_audio_data = np.array([], dtype=np.float32)
            else:
              print("No speech detected, clearing audio buffer")
              if args.no_faster_whisper:
                acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
              else:
                acc_audio_data = np.array([], dtype=np.float32)

          except Exception as e:
            print(f"Error in transcription process: {e}")
            # æ¸…ç©ºéŸ³é¢‘ç¼“å†²åŒºï¼Œé¿å…é‡å¤å¤„ç†é”™è¯¯çš„æ•°æ®
            if args.no_faster_whisper:
              acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
            else:
              acc_audio_data = np.array([], dtype=np.float32)
            sleep(0.3)
            continue

          # ç®€åŒ–çš„åå¤„ç†é€»è¾‘
          if args.stabilize_turns > 0:
            if len(texts) == 0 and len(acc_audio_data)/self.sample_rate > args.max_duration:
              cut_off = int(len(acc_audio_data) - args.min_duration*self.sample_rate)
              acc_audio_data = acc_audio_data[cut_off:]

            pos = 0
            while pos < min(len(last_texts), len(texts))-args.stabilize_turns:
              if last_texts[pos] != texts[pos]:
                break
              pos += 1

            if pos <= 0 and len(texts) > 1:
              pos = 0
              while len(acc_audio_data)/self.sample_rate - result['segments'][pos]['end'] > args.max_duration:
                pos += 1

            if pos > 0:
              seg = result['segments'][pos-1]
              cut_off = int(seg['end']*self.sample_rate)
              cut_off = min(cut_off, int(len(acc_audio_data)-args.min_duration*self.sample_rate))
              cut_off = max(0, cut_off)
              acc_audio_data = acc_audio_data[cut_off:]
              texts = texts[pos:]

              transcription += last_texts[:pos]
          else:
            if phrase_cut_off > 0:
              transcription += last_texts

          if not args.keep_transcriptions:
            transcription = transcription[-self.max_transcription_history:]

          last_texts = texts

          # æ¸…ç©ºæ§åˆ¶å°å¹¶é‡æ–°æ‰“å°æ›´æ–°çš„è½¬å½•
          os.system('cls' if os.name=='nt' else 'clear')
          for line in transcription:
            print(line)
          # åªåœ¨æœ‰segmentsçš„æƒ…å†µä¸‹æ‰“å°è°ƒè¯•ä¿¡æ¯
          if texts:
            print("Transcription successful:", texts)
          # åˆ·æ–°æ ‡å‡†è¾“å‡º
          print('', end='', flush=True)
          
        except KeyboardInterrupt:
          break
        except Exception as e:
          print(f"Error in transcription loop: {e}")
          import traceback
          traceback.print_exc()
          sleep(0.3)
          continue

      # æœ€ç»ˆç»“æœ
      transcription += last_texts

      print("\n\nFinal Transcription:")
      for line in transcription:
        print(line)

    except Exception as e:
      print(f"Critical error in transcription thread: {e}")
      import traceback
      traceback.print_exc()
    finally:
      # å§‹ç»ˆæ¸…ç†èµ„æº
      try:
        self.input_provider.stop_record()
      except Exception as e:
        print(f"Error stopping recording: {e}")

    return transcription

def main():
  args = parse_args()

  try:
    # é¦–å…ˆåœ¨ä¸»çº¿ç¨‹åˆ›å»ºQApplication
    app = QApplication([])
    
    print("Creating caption display window...")
    
    # åˆ›å»ºHUDçª—å£
    hud_window = HUD(font_size=args.font_size)
    hud_window.show()
    
    # å¼ºåˆ¶çª—å£æ˜¾ç¤ºåœ¨å‰å°
    hud_window.raise_()
    hud_window.activateWindow()
    
    print("Caption window displayed")
    
    # å…¨å±€å˜é‡åˆå§‹å†…å®¹
    global text_to_display
    with text_lock:
      text_to_display = "ğŸ¤ å®æ—¶è½¬å½•ç³»ç»Ÿå·²å¯åŠ¨\nè¯·å¼€å§‹è¯´è¯..."
    
    print("Creating transcriber...")
    
    # åˆ›å»ºè½¬å½•å™¨
    transcriber = Transcriber(args)
    
    print("Starting transcription thread...")
    
    # å¯åŠ¨è½¬å½•çº¿ç¨‹
    transcriber.start_transcribe_thread()

    # è®¾ç½®ä¿¡å·å¤„ç†
    def handle_signal(sig, frame):
      print("\nReceived interrupt signal, shutting down...")
      transcriber.stop_transcribe_thread()
      app.quit()
    signal.signal(signal.SIGINT, handle_signal)

    print("Starting UI event loop...")

    # è¿è¡Œäº‹ä»¶å¾ªç¯
    app.exec()

    # åœæ­¢è½¬å½•çº¿ç¨‹
    transcriber.stop_transcribe_thread()
    
  except KeyboardInterrupt:
    print("\nProgram interrupted by user. Exiting...")
  except Exception as e:
    print(f"Critical error in main function: {e}")
    import traceback
    traceback.print_exc()

if __name__ == "__main__":
  main()