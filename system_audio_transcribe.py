#! python3.7

import sounddevice
import argparse
import os
import numpy as np
import speech_recognition as sr
import whisper
import torch
import threading
import pyaudio
import signal
import time
from faster_whisper import WhisperModel

from datetime import datetime, timedelta
from queue import Queue
from time import sleep
from sys import platform

from PyQt5.QtWidgets import QApplication, QMainWindow, QWidget, QHBoxLayout, QLabel, QTextEdit
from PyQt5.QtGui import QFont, QFontMetrics, QCursor
from PyQt5.QtCore import QChildEvent, Qt, QTimer, QPoint, QRect, QSize, pyqtSignal, QObject

# å…¨å±€å…±äº«å˜é‡ç”¨äºçº¿ç¨‹é—´é€šä¿¡
text_to_display = "Loading captions..."
text_lock = threading.Lock()

def parse_args():
  parser = argparse.ArgumentParser(
    description="ğŸ”Š Real-time System Audio Transcription with Live Subtitles",
    formatter_class=argparse.RawDescriptionHelpFormatter,
    epilog="""
ğŸš€ Quick Start Examples:
  python system_audio_transcribe.py                    # Start with default settings (tiny.en model, BlackHole audio)
  python system_audio_transcribe.py --model base.en    # Use base model for better accuracy
  python system_audio_transcribe.py --language zh      # Transcribe Chinese audio
  python system_audio_transcribe.py --font-size 40     # Larger subtitle font

ğŸ“ For more help: https://github.com/jiji262/realtime-transcribe
    """
  )

  # ç®€åŒ–çš„æ ¸å¿ƒå‚æ•°ï¼Œè®¾ç½®æœ€ä½³é»˜è®¤å€¼
  parser.add_argument("--model", default="tiny.en", help="Whisper model to use (default: tiny.en for fast English transcription)",
            choices=["tiny", "base", "small", "medium", "large", "tiny.en", "base.en", "small.en", "medium.en", "large-v3"])
  parser.add_argument("--language", default="en",
            help="Language for transcription (default: en). Use 'zh' for Chinese, 'auto' for auto-detection", type=str)
  parser.add_argument("--font-size", default=32,
            help="Subtitle font size (default: 32)", type=int)

  # é«˜çº§å‚æ•°ï¼ˆå¤§å¤šæ•°ç”¨æˆ·ä¸éœ€è¦ä¿®æ”¹ï¼‰
  parser.add_argument("--input", default=None,
            help="Audio input device (auto-detects BlackHole by default)", type=str)
  parser.add_argument("--input-provider", default="pyaudio",
            choices=["pyaudio", "speech-recognition"],
            help="Audio input provider (default: pyaudio)", type=str)
  parser.add_argument("--no-faster-whisper", action='store_true', default=True,
            help="Use standard Whisper instead of faster-whisper (default: enabled for stability)")

  parser.add_argument("--translate", action='store_true', default=False,
            help="Translate to English")

  parser.add_argument("--no-fp16", action='store_true', default=False,
            help="Disable fp16 optimization")
  parser.add_argument("--stabilize-turns", default=0,
            help="Turns to stabilize result (default: 0 for real-time)", type=int)
  parser.add_argument("--min-duration", default=0.2,
            help="Min duration of audio to process (default: 0.2s for low latency)", type=float)
  parser.add_argument("--max-duration", default=1.5,
            help="Max duration of audio to process (default: 1.5s for low latency)", type=float)
  parser.add_argument("--keep-transcriptions", action='store_true', default=False,
            help="Keep all previous transcriptions")

  # args for input provider 'speech-recognition'
  parser.add_argument("--energy_threshold", default=300,
            help="Energy level for mic to detect", type=int)
  parser.add_argument("--record_timeout", default=0.3,
            help="How real time the recording is in seconds", type=float)
  parser.add_argument("--phrase_timeout", default=0.8,
            help="How much empty space between recordings before considering it a new line", type=float)

  # args for input provider 'pyaudio'
  parser.add_argument("--moving-window", default=10,
            help="Moving window duration in seconds", type=int)
  parser.add_argument("--chunk-size", default=512,
            help="Audio chunk size (default: 512 for low latency)", type=int)
  parser.add_argument("--realtime-mode", action='store_true', default=True,
            help="Enable real-time optimizations (default: enabled)")
  args = parser.parse_args()
  return args


class HUDText(QTextEdit):
  def __init__(self, font_size):
    super().__init__()

    self.setReadOnly(True)
    self.setAlignment(Qt.AlignLeft|Qt.AlignVCenter)
    self.setStyleSheet("""
      background-color: qlineargradient(x1:0, y1:0, x2:0, y2:1,
                                        stop:0 rgba(20,20,20,0.8), 
                                        stop:1 rgba(40,40,40,0.8));
      color: #FFFFFF;
      border-radius: 15px;
      border: 1px solid rgba(255,255,255,0.3);
      padding: 10px;
      selection-background-color: rgba(30,144,255,0.5);
      selection-color: white;
    """)
    self.setLineWrapMode(QTextEdit.WidgetWidth)
    
    # è®¾ç½®å­—ä½“
    font = QFont("Arial", font_size)
    font.setWeight(QFont.Medium)
    self.setFont(font)
    
    # è®¾ç½®æ–‡æœ¬æ ¼å¼
    self.document().setDefaultStyleSheet("""
      p { margin-bottom: 10px; line-height: 120%; }
      .current { color: #F8F8F8; }
      .previous { color: rgba(255,255,255,0.75); }
    """)
    
    self.setVerticalScrollBarPolicy(Qt.ScrollBarAlwaysOff)
    self.viewport().setCursor(Qt.CursorShape.ArrowCursor)
    
    # è®¾ç½®æ–‡æ¡£è¾¹è·
    document = self.document()
    document.setDocumentMargin(15)

  def mousePressEvent(self, event):
    event.ignore()

  def mouseMoveEvent(self, event):
    event.ignore()

  def mouseReleaseEvent(self, event):
    event.ignore()

class HUD(QMainWindow):
  max_width_percentage = 0.7  # å¢å¤§çª—å£å®½åº¦ï¼Œé¿å…æ–‡æœ¬æ¢è¡Œè¿‡å¤š
  max_height_percentage = 0.35  # å†å¢åŠ é«˜åº¦ä»¥å®¹çº³æ›´å¤šæ–‡æœ¬
  max_lines = 6
  padding = 15
  corner_spacing = 20

  def __init__(self, font_size):
    super().__init__()

    # è®¾ç½®çª—å£å±æ€§
    self.setWindowFlags(Qt.CustomizeWindowHint|Qt.FramelessWindowHint|Qt.WindowStaysOnTopHint|Qt.WindowDoesNotAcceptFocus)
    self.setAttribute(Qt.WA_TranslucentBackground)

    # è®¾ç½®çª—å£é€æ˜åº¦
    self.setWindowOpacity(1.0)

    # è®¾ç½®ä¸­å¤®çª—å£
    central_widget = QWidget()
    layout = QHBoxLayout(central_widget)
    layout.setContentsMargins(10, 10, 10, 10)

    self.text_widget = HUDText(font_size)
    self.text_widget.setParent(central_widget)
    layout.addWidget(self.text_widget)

    self.setCentralWidget(central_widget)

    # é™åˆ¶çª—å£å¤§å°
    screen_geometry = QApplication.desktop().screenGeometry()
    max_width = int(self.max_width_percentage * screen_geometry.width())
    max_height = int(self.max_height_percentage * screen_geometry.height())
    self.setFixedWidth(max_width)
    self.setFixedHeight(max_height)
    
    # å°†çª—å£æ”¾åœ¨å±å¹•åº•éƒ¨ä¸­å¤®
    self.move(int((screen_geometry.width() - max_width) / 2), 
              screen_geometry.height() - max_height - self.corner_spacing)

    # å®šæœŸæ›´æ–°æ–‡æœ¬çš„å®šæ—¶å™¨
    self.update_text_timer = QTimer(self)
    self.update_text_timer.timeout.connect(self.updateTextWidget)
    self.update_text_timer.start(33)  # çº¦30fpsçš„æ›´æ–°é¢‘ç‡

    # æ‹–åŠ¨çª—å£çš„å˜é‡
    self.old_pos = None
    
    # ä¸Šä¸€æ¬¡æ˜¾ç¤ºçš„æ–‡æœ¬
    self.last_displayed_text = ""
    
    # åˆå§‹æµ‹è¯•æ–‡æœ¬
    global text_to_display
    with text_lock:
      text_to_display = "System audio caption window started\nStart playing audio..."
    
    print("HUD window created, initial text set")

  def mousePressEvent(self, event):
    if event.button() == Qt.LeftButton:
      self.old_pos = event.globalPos()

  def mouseMoveEvent(self, event):
    if self.old_pos:
      delta = QPoint(event.globalPos() - self.old_pos)
      self.move(self.x() + delta.x(), self.y() + delta.y())
      self.old_pos = event.globalPos()

  def mouseReleaseEvent(self, event):
    if event.button() == Qt.LeftButton:
      self.old_pos = None

  def updateTextWidget(self):
    # ä»å…¨å±€å˜é‡è·å–å½“å‰æ–‡æœ¬
    global text_to_display
    with text_lock:
      current_text = text_to_display
    
    # è°ƒè¯•ä¿¡æ¯ï¼Œæ˜¾ç¤ºå½“å‰æ–‡æœ¬
    if current_text:
      print(f"Current text_to_display: '{current_text[:30]}...' (len={len(current_text)})")
    else:
      print("Current text_to_display is empty")
    
    # åªåœ¨æ–‡æœ¬éç©ºä¸”ä¸ä¸Šæ¬¡ä¸åŒæ—¶æ›´æ–°ï¼Œå¹¶ä¸”ç¡®ä¿æœ€å°é•¿åº¦ï¼Œé¿å…æ˜¾ç¤ºè¿‡çŸ­çš„ç‰‡æ®µ
    if current_text and current_text.strip() and current_text != self.last_displayed_text:
      try:
        # å¤„ç†æ–‡æœ¬ï¼Œç¡®ä¿æ¯å¥è¯æ¢è¡Œæ˜¾ç¤º
        formatted_text = self.format_text(current_text)
        
        # ä½¿ç”¨HTMLæ ¼å¼åŒ–æ–‡æœ¬
        html_text = self.format_text_html(formatted_text)
        
        # æ›´æ–°æ˜¾ç¤ºæ–‡æœ¬
        self.text_widget.setHtml(html_text)
        
        # è®°å½•æ˜¾ç¤ºçš„æ–‡æœ¬
        self.last_displayed_text = current_text
        
        # è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
        vertical_scrollbar = self.text_widget.verticalScrollBar()
        vertical_scrollbar.setValue(vertical_scrollbar.maximum())

        # è°ƒè¯•ä¿¡æ¯
        print(f"HUD text updated successfully, length: {len(current_text)}")
        
        # å¼ºåˆ¶é‡ç»˜çª—å£
        self.text_widget.repaint()
      except Exception as e:
        print(f"Error updating caption window: {e}")
        import traceback
        traceback.print_exc()

  def format_text(self, text):
    # å¤„ç†æ–‡æœ¬ï¼Œç¡®ä¿æ¯å¥è¯éƒ½æ¢è¡Œ
    # é¦–å…ˆæŒ‰åŸæœ‰çš„æ¢è¡Œç¬¦åˆ†å‰²
    paragraphs = text.split('\n')
    formatted_paragraphs = []
    
    for paragraph in paragraphs:
      # å¤„ç†æ®µè½å†…çš„å¥å­
      # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·ç­‰åˆ†å‰²å¥å­ï¼Œä¿ç•™åˆ†éš”ç¬¦
      sentences = []
      current_pos = 0
      for i, char in enumerate(paragraph):
        if char in '.!?ã€‚ï¼ï¼Ÿ':
          if i+1 < len(paragraph) and paragraph[i+1] != ' ':
            # å¦‚æœå¥å·åé¢æ²¡æœ‰ç©ºæ ¼ï¼Œå¯èƒ½æ˜¯ç¼©å†™æˆ–å°æ•°ç‚¹ï¼Œä¸åˆ†å‰²
            continue
          if i+1 < len(paragraph):
            sentences.append(paragraph[current_pos:i+2])
            current_pos = i+2
          else:
            sentences.append(paragraph[current_pos:i+1])
            current_pos = i+1
      
      # æ·»åŠ æœ€åä¸€å¥ï¼ˆå¦‚æœæœ‰ï¼‰
      if current_pos < len(paragraph):
        sentences.append(paragraph[current_pos:])
      
      # å°†å¤„ç†åçš„å¥å­æ·»åŠ åˆ°æ ¼å¼åŒ–æ®µè½
      if sentences:
        formatted_paragraphs.append('\n'.join(sentences))
      else:
        formatted_paragraphs.append(paragraph)
    
    # å°†æ ¼å¼åŒ–çš„æ®µè½ç»„åˆæˆæœ€ç»ˆæ–‡æœ¬
    return '\n\n'.join(formatted_paragraphs)
  
  def format_text_html(self, text):
    # å°†æ–‡æœ¬è½¬æ¢ä¸ºHTMLæ ¼å¼ï¼Œå¢å¼ºå¯è¯»æ€§
    lines = text.split('\n')
    html_parts = ['<html><body>']

    # æ·»åŠ é¡¶éƒ¨è¾¹è·
    html_parts.append('<div style="margin-top:5px;"></div>')

    for i, line in enumerate(lines):
      if line.strip():
        # æœ€åä¸€è¡Œï¼ˆå½“å‰æ­£åœ¨è½¬å½•çš„ï¼‰ä½¿ç”¨è¾ƒäº®çš„é¢œè‰²
        # ä¹‹å‰çš„è¡Œä½¿ç”¨ç¨æš—çš„é¢œè‰²è¡¨ç¤ºå·²å®Œæˆ
        if i == len(lines) - 1:
          # å½“å‰æ­£åœ¨è½¬å½•çš„å¥å­ - ä½¿ç”¨åŠ¨ç”»æ•ˆæœå‡å°‘è·³è·ƒæ„Ÿ
          html_parts.append(f'<p class="current" style="line-height:130%; margin-bottom:8px; letter-spacing:0.5px; color:#F8F8F8; transition: all 0.3s ease-in-out; opacity: 1; transform: translateY(0);">{line}</p>')
        else:
          # å·²å®Œæˆçš„å¥å­ - ç¨æš—ä½†ç¨³å®š
          html_parts.append(f'<p class="previous" style="line-height:130%; margin-bottom:8px; letter-spacing:0.5px; color:rgba(255,255,255,0.85); transition: all 0.3s ease-in-out; opacity: 0.9;">{line}</p>')

    html_parts.append('</body></html>')
    return ''.join(html_parts)

class AudioInputProvider:
  def list_input_devices(self):
    raise NotImplementedError

  def init_input_device(self, device_index):
    raise NotImplementedError

  def start_record(self):
    raise NotImplementedError

  def stop_record(self):
    raise NotImplementedError

  def phrase_cut_off(self, acc_data, new_data):
    raise NotImplementedError

class SystemAudioProvider(AudioInputProvider):
  def __init__(self, args, data_queue, sample_rate):
    self.audio = pyaudio.PyAudio()

    self.audio_format = pyaudio.paFloat32  # æ”¹ä¸ºFloat32æ ¼å¼ï¼Œä¸è¯Šæ–­å·¥å…·ä¸€è‡´
    self.audio_channels = 1  # Number of audio channels (1 for mono, 2 for stereo)
    self.sample_rate = sample_rate  # Sample rate (samples per second)
    self.sample_size = self.audio.get_sample_size(self.audio_format)
    self.audio_chunk = args.chunk_size

    self.moving_window = args.moving_window

    self.device_index = None
    self.stop_event = threading.Event()
    self.stream = None  # Add stream reference for proper cleanup

    self.data_queue = data_queue
    print(f"SystemAudioProvider initialized successfully, sample rate: {sample_rate}Hz, chunk size: {args.chunk_size}")

  def __del__(self):
    """Ensure PyAudio is properly terminated when object is destroyed"""
    try:
      if hasattr(self, 'audio') and self.audio:
        # Only terminate if not already terminated
        if hasattr(self.audio, '_pa'):
          self.audio.terminate()
          self.audio = None
    except Exception as e:
      print(f"Error terminating PyAudio: {e}")

  def list_input_devices(self):
    devices = []
    self.device_index_map = {}  # æ˜ å°„åˆ—è¡¨ç´¢å¼•åˆ°å®é™…PyAudioè®¾å¤‡ç´¢å¼•
    try:
      print("Finding available audio input devices...")
      list_idx = 0
      for pyaudio_idx in range(self.audio.get_device_count()):
        device_info = self.audio.get_device_info_by_index(pyaudio_idx)
        # Only include devices that support input
        if device_info.get('maxInputChannels', 0) > 0:
          devices.append(device_info['name'])
          self.device_index_map[list_idx] = pyaudio_idx  # ä¿å­˜æ˜ å°„å…³ç³»
          print(f"Found input device {list_idx}: {device_info['name']} (PyAudio index: {pyaudio_idx})")
          list_idx += 1

      # If no input devices found, add a default option
      if not devices:
        print("No input devices found, using default device")
        devices.append("Default Input Device")
        self.device_index_map[0] = None
    except Exception as e:
      print(f"Error listing audio devices: {e}")
      devices.append("Default Input Device")
      self.device_index_map[0] = None

    return devices

  def init_input_device(self, device_list_index):
    # å°†è®¾å¤‡åˆ—è¡¨ç´¢å¼•è½¬æ¢ä¸ºå®é™…çš„PyAudioè®¾å¤‡ç´¢å¼•
    if hasattr(self, 'device_index_map') and device_list_index in self.device_index_map:
      actual_device_index = self.device_index_map[device_list_index]
      print(f"Mapping device list index {device_list_index} to PyAudio index {actual_device_index}")
    else:
      actual_device_index = device_list_index
      print(f"No mapping found, using device index {device_list_index} directly")

    # Validate the device supports input
    try:
      print(f"Initializing audio device, PyAudio index: {actual_device_index}")
      if actual_device_index is not None:
        device_info = self.audio.get_device_info_by_index(actual_device_index)
        if device_info.get('maxInputChannels', 0) <= 0:
          print(f"Warning: Device {actual_device_index} does not support input, attempting to use default input device")
          actual_device_index = self.audio.get_default_input_device_info()['index']
    except Exception as e:
      print(f"Error with device {actual_device_index}: Using default device: {e}")
      actual_device_index = None

    self.device_index = actual_device_index
    print(f"Using input device index: {self.device_index}")

  def start_record(self):
    print("Starting SystemAudio recording thread...")
    self.stop_event.clear()

    self.record_thread = threading.Thread(target=self._record_audio)
    self.record_thread.daemon = True  # Make thread daemon so it exits when main program exits
    self.record_thread.start()
    print("SystemAudio recording thread started")

  def stop_record(self):
    try:
      print("Stopping SystemAudio recording...")
      self.stop_event.set()

      # Close stream if it exists
      if hasattr(self, 'stream') and self.stream:
        try:
          if self.stream.is_active():
            self.stream.stop_stream()
          self.stream.close()
          self.stream = None
        except Exception as e:
          print(f"Error closing audio stream: {e}")

      if hasattr(self, 'record_thread') and self.record_thread.is_alive():
        self.record_thread.join(timeout=2.0)  # Wait up to 2 seconds for thread to finish
        print("SystemAudio recording stopped")

      # Terminate PyAudio
      if hasattr(self, 'audio') and self.audio:
        try:
          self.audio.terminate()
          self.audio = None
        except Exception as e:
          print(f"Error terminating PyAudio: {e}")

    except Exception as e:
      print(f"Error stopping recording: {e}")

  def phrase_cut_off(self, acc_data, new_data):
    if (exceed := len(acc_data) + len(new_data) - self.sample_rate*self.moving_window) > 0:
      return exceed
    else:
      return 0

  def _record_audio(self):
    def stream_callback(in_data, frame_count, time_info, status):
      # Add small visual feedback that we're receiving audio
      data_size = len(in_data)
      if data_size > 0:
        # éŸ³é¢‘æ•°æ®æœ‰æ•ˆï¼Œæ”¾å…¥é˜Ÿåˆ—
        self.data_queue.put(in_data)
        # è§†è§‰åé¦ˆï¼Œä½†é™åˆ¶æ‰“å°é¢‘ç‡
        print(".", end="", flush=True)
      else:
        print("x", end="", flush=True)  # æ”¶åˆ°ç©ºæ•°æ®æ—¶æ˜¾ç¤ºx

      # è¿”å›Noneè¡¨ç¤ºæ— è¾“å‡ºæ•°æ®ï¼Œpyaudio.paContinueè¡¨ç¤ºç»§ç»­å½•åˆ¶
      return (None, pyaudio.paContinue)

    try:
      # å°è¯•å¤šæ¬¡åˆå§‹åŒ–éŸ³é¢‘æµ
      max_attempts = 3
      attempts = 0
      stream = None

      while attempts < max_attempts and not self.stop_event.is_set():
        attempts += 1
        try:
          # Open audio stream with the selected input device
          if self.device_index is None:
            # Use default input device if none specified
            print(f"Attempt {attempts}/{max_attempts}: Using default input device")
            stream = self.audio.open(
              format=self.audio_format,
              channels=self.audio_channels,
              rate=self.sample_rate,
              input=True,
              frames_per_buffer=self.audio_chunk,
              stream_callback=stream_callback,
            )
          else:
            # Use specified input device
            print(f"Attempt {attempts}/{max_attempts}: Using device index {self.device_index}")
            stream = self.audio.open(
              format=self.audio_format,
              channels=self.audio_channels,
              rate=self.sample_rate,
              input=True,
              input_device_index=self.device_index,
              frames_per_buffer=self.audio_chunk,
              stream_callback=stream_callback,
            )

          # æˆåŠŸæ‰“å¼€æµ
          print(f"Audio stream started, device index: {self.device_index}")
          break
        except Exception as e:
          print(f"Error opening audio stream (attempt {attempts}/{max_attempts}): {e}")
          if attempts < max_attempts:
            print("Retrying in 1 second...")
            time.sleep(1)
            if self.device_index is not None and attempts == max_attempts - 1:
              print("Trying with default device as last resort")
              self.device_index = None
          else:
            raise

      if not stream or not stream.is_active():
        print("CRITICAL ERROR: Failed to open an active audio stream")
        return

      # ç›´æ¥æµ‹è¯•éŸ³é¢‘æ•è·
      print("Testing system audio capture for 2 seconds...")
      test_start = time.time()
      while time.time() - test_start < 2.0 and not self.stop_event.is_set():
        if self.data_queue.qsize() > 0:
          print(f"\nSystem audio capture test successful! Queue size: {self.data_queue.qsize()}")
          break
        sleep(0.1)

      if self.data_queue.qsize() == 0:
        print("\nWARNING: No system audio data received during test. Check your BlackHole setup.")

      # Store stream reference for cleanup
      self.stream = stream

      # Keep the stream active until stop is requested
      while not self.stop_event.is_set() and stream.is_active():
        # æ¯éš”ä¸€æ®µæ—¶é—´æŠ¥å‘Šä¸€ä¸‹é˜Ÿåˆ—å¤§å°
        if self.data_queue.qsize() > 0 and self.data_queue.qsize() % 20 == 0:
          print(f"\nAudio queue size: {self.data_queue.qsize()}")
        sleep(0.1)

      # Close the audio stream
      if stream:
        try:
          stream.stop_stream()
          stream.close()
          self.stream = None
          print("Audio stream closed")
        except Exception as e:
          print(f"Error closing stream: {e}")

    except Exception as e:
      print(f"Critical error in system audio recording: {e}")
      import traceback
      traceback.print_exc()
      # No automatic retry with default device, let the error bubble up

class SystemAudioTranscriber():
  n_context = 5
  max_transcription_history = 100

  def __init__(self, args):
    self.args = args
    self.language = args.language
    self.sample_rate = whisper.audio.SAMPLE_RATE
    # Use CPU by default for more compatibility, allow opt-in to GPU
    self.compute_device = "cpu"
    if torch.cuda.is_available():
      print("CUDA is available! To use GPU, restart the program without --no-faster-whisper flag")

    self.model_name = args.model

    # Thread safe Queue for passing data from the threaded recording callback.
    self.data_queue = Queue()
    self.transcribe_thread = None
    self.stop_event = threading.Event()

    # Use SystemAudioProvider for capturing system audio
    self.input_provider = SystemAudioProvider(args=self.args, data_queue=self.data_queue, sample_rate=self.sample_rate)

    print(f"Using SystemAudioProvider for system audio capture")
    print(f"Using {self.model_name} model")
    print(f"Computing on {self.compute_device}")

    self.init_input_device(args)

    print(f"Loading model {self.model_name}...")
    # Load / Download model
    start_time = time.time()
    try:
      if self.args.no_faster_whisper:
        self.audio_model = whisper.load_model(self.model_name, device=self.compute_device)
      else:
        self.audio_model = WhisperModel(self.model_name, device=self.compute_device)
      print(f"Model loaded in {time.time() - start_time:.2f} seconds")
    except Exception as e:
      print(f"Error loading model: {e}")
      raise

    # Cue the user that we're ready to go.
    print("System ready. Starting system audio transcription...\n")

  def init_input_device(self, args):
    device_index = None

    if args.input is not None:
      try:
        # æ£€æŸ¥æ˜¯å¦è¾“å…¥çš„æ˜¯æ•°å­—ç´¢å¼•
        if args.input.isdigit():
          device_index = int(args.input)
          devices = self.input_provider.list_input_devices()
          if device_index < 0 or device_index >= len(devices):
            print(f"Warning: Device index {device_index} out of range, will list available devices")
            device_index = None
          else:
            print(f"Using specified device index: {device_index} ({devices[device_index]})")
        else:
          # å¦‚æœä¸æ˜¯æ•°å­—ï¼Œå°è¯•åŒ¹é…è®¾å¤‡åç§°
          for idx, name in enumerate(self.input_provider.list_input_devices()):
            if args.input.lower() in name.lower():  # ä½¿ç”¨éƒ¨åˆ†åŒ¹é…è€Œä¸æ˜¯ç²¾ç¡®åŒ¹é…
              device_index = idx
              print(f"Found matching device: {idx}. {name}")
              break
      except Exception as e:
        print(f"Error finding specified input device: {e}")
        device_index = None

    if device_index is None:
      try:
        # æ‰“å°å¯ç”¨éŸ³é¢‘è®¾å¤‡åˆ—è¡¨
        print("Available input devices:")
        devices = list(self.input_provider.list_input_devices())
        for idx, name in enumerate(devices):
          print(f"{idx}. {name}")

        if args.input is None:
          # é€‰æ‹©æ‰€éœ€è¾“å…¥è®¾å¤‡çš„ç´¢å¼• - ä¼˜å…ˆæŸ¥æ‰¾BlackHole
          default_device = 0
          # ä¼˜å…ˆæŸ¥æ‰¾BlackHoleè®¾å¤‡
          for idx, name in enumerate(devices):
            if "BlackHole" in name or "blackhole" in name.lower():
              default_device = idx
              print(f"Found BlackHole device: {idx}. {name}")
              break
          # å¦‚æœæ²¡æ‰¾åˆ°BlackHoleï¼ŒæŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„è™šæ‹ŸéŸ³é¢‘è®¾å¤‡
          if default_device == 0:
            for idx, name in enumerate(devices):
              if any(keyword in name.lower() for keyword in ["virtual", "loopback", "soundflower"]):
                default_device = idx
                print(f"Found virtual audio device: {idx}. {name}")
                break

          print(f"Default device will be {default_device}: {devices[default_device]} in 5 seconds...")
          print("Enter device number to override (or press Enter to use default):", end="", flush=True)

          # è®¾ç½®è¶…æ—¶è¯»å–è¾“å…¥
          import select
          import sys

          # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·è¾“å…¥ï¼Œæœ€å¤šç­‰å¾…5ç§’
          ready, _, _ = select.select([sys.stdin], [], [], 5)
          if ready:
            user_input = sys.stdin.readline().strip()
            if user_input.isdigit() and 0 <= int(user_input) < len(devices):
              device_index = int(user_input)
              print(f"User selected device: {device_index}. {devices[device_index]}")
            else:
              if user_input:
                print(f"Invalid input '{user_input}', using default device")
              device_index = default_device
          else:
            print("\nNo input received, using default device")
            device_index = default_device
        else:
          print(f"Could not find device matching '{args.input}', please select from available devices")
          return self.init_input_device(argparse.Namespace(**{**vars(args), 'input': None}))  # é‡æ–°è°ƒç”¨ä½†æ¸…é™¤inputå‚æ•°
      except Exception as e:
        print(f"Error in device selection: {e}")
        device_index = None  # ä½¿ç”¨é»˜è®¤è®¾å¤‡

    try:
      self.input_provider.init_input_device(device_index)
    except Exception as e:
      print(f"Critical error initializing input device: {e}")
      raise

  def start_transcribe_thread(self):
    if self.transcribe_thread is not None:
      print("Warning: Transcription thread already running")
      return

    self.transcribe_thread = threading.Thread(target=self.listen)
    self.transcribe_thread.daemon = True
    self.transcribe_thread.start()
    print("Transcription thread started")

  def stop_transcribe_thread(self):
    if self.transcribe_thread is None:
      return

    print("Stopping transcription thread...")
    self.stop_event.set()
    self.transcribe_thread.join(timeout=5.0)  # Wait up to 5 seconds
    print("Transcription thread stopped")

  def update_hud_text(self, text):
    # æ›´æ–°å…¨å±€æ–‡æœ¬å˜é‡
    global text_to_display
    with text_lock:
      # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
      if text is None:
        print("Warning: Attempted to update with None text, ignored")
        return

      if not isinstance(text, str):
        text = str(text)
        print(f"Warning: Non-string text converted to: {text}")

      # å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦ä½†ä¿ç•™åŸºæœ¬æ ¼å¼
      cleaned_text = ' '.join([line.strip() for line in text.split('\n')])

      # ç¡®ä¿æ–‡æœ¬éç©º
      if cleaned_text.strip():
        # æ›´æ–°å…¨å±€å˜é‡
        text_to_display = cleaned_text
        # é™åˆ¶æ—¥å¿—é•¿åº¦ä»¥é¿å…åˆ·å±
        preview = cleaned_text[:50] + "..." if len(cleaned_text) > 50 else cleaned_text
        print(f"Updated global text_to_display: '{preview}' (len={len(cleaned_text)})")

        # Note: UI updates are handled by the main thread timer, no need to force update here
      else:
        print("Warning: Attempted to update with empty text, ignored")

  def manage_caption_history(self, new_texts, caption_history, current_caption, last_complete_caption, max_lines=5):
    """
    æ™ºèƒ½ç®¡ç†å­—å¹•å†å²ï¼Œé˜²æ­¢è·³è·ƒï¼Œæ”¯æŒå¤šè¡Œæ»šåŠ¨æ˜¾ç¤º
    è¿”å›: (updated_history, updated_current, updated_last_complete, display_text)
    """
    if not new_texts:
      return caption_history, current_caption, last_complete_caption, None

    # è·å–æœ€æ–°çš„è½¬å½•æ–‡æœ¬
    latest_text = new_texts[-1] if new_texts else ""
    latest_text = latest_text.strip()

    if not latest_text:
      return caption_history, current_caption, last_complete_caption, None

    # æ€»æ˜¯æ·»åŠ æ–°çš„è½¬å½•ç»“æœåˆ°å†å²ä¸­ï¼ˆä¸ç®¡æ˜¯å¦å®Œæ•´ï¼‰
    if latest_text and latest_text != last_complete_caption:
      # é¿å…é‡å¤æ·»åŠ ç›¸åŒçš„æ–‡æœ¬
      if not caption_history or caption_history[-1] != latest_text:
        caption_history.append(latest_text)
        last_complete_caption = latest_text
        print(f"Added new caption: {latest_text}")

        # ä¿æŒæœ€å¤§è¡Œæ•°é™åˆ¶ï¼Œå®ç°å‘ä¸Šæ»šåŠ¨æ•ˆæœ
        if len(caption_history) > max_lines:
          caption_history = caption_history[-max_lines:]
          print(f"Caption history trimmed to {max_lines} lines")

    # æ„å»ºæ˜¾ç¤ºæ–‡æœ¬ - æ˜¾ç¤ºæ‰€æœ‰å†å²è®°å½•
    display_lines = []
    for sentence in caption_history:
      if sentence.strip():
        display_lines.append(sentence.strip())

    # ç”Ÿæˆæœ€ç»ˆæ˜¾ç¤ºæ–‡æœ¬
    display_text = '\n'.join(display_lines) if display_lines else None

    return caption_history, current_caption, last_complete_caption, display_text

  def listen(self):
    args = self.args
    transcription = []
    last_texts = []
    texts = []  # Initialize texts variable to prevent UnboundLocalError
    result = {'segments': []}  # Initialize result variable to prevent UnboundLocalError
    realtime_mode = args.realtime_mode

    # å­—å¹•å†å²ç®¡ç† - é˜²æ­¢è·³è·ƒ
    caption_history = []  # å·²å®Œæˆçš„å­—å¹•å¥å­
    current_caption = ""  # å½“å‰æ­£åœ¨è½¬å½•çš„å¥å­
    max_caption_lines = 5  # æœ€å¤šæ˜¾ç¤ºçš„å­—å¹•è¡Œæ•°
    last_complete_caption = ""  # ä¸Šæ¬¡çš„å®Œæ•´å­—å¹•ï¼Œç”¨äºæ£€æµ‹æ–°å¥å­

    # å­—å¹•æ˜¾ç¤ºç¨³å®šæ€§æ§åˆ¶
    last_transcription_result = ""  # ä¸Šæ¬¡çš„è½¬å½•ç»“æœ
    last_result_display_time = 0  # ä¸Šæ¬¡æ˜¾ç¤ºç»“æœçš„æ—¶é—´
    result_display_duration = 5.0  # è½¬å½•ç»“æœæ˜¾ç¤ºæŒç»­æ—¶é—´ï¼ˆç§’ï¼‰- å¢åŠ åˆ°5ç§’ï¼Œæé«˜ç¨³å®šæ€§
    is_showing_result = False  # æ˜¯å¦æ­£åœ¨æ˜¾ç¤ºè½¬å½•ç»“æœ

    try:
      print("Starting system audio recording...")
      self.input_provider.start_record()
      print("System audio recording started, waiting for audio data")

      if args.no_faster_whisper:
        import torch
        acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
        last_transcription_time = time.time()
        last_audio_debug_time = time.time()

        # ç¡®ä¿å¯åŠ¨æ—¶æ›´æ–°UI
        self.update_hud_text("ğŸ”Š æ­£åœ¨ç›‘å¬ç³»ç»ŸéŸ³é¢‘...\næ’­æ”¾éŸ³é¢‘å†…å®¹ä»¥å¼€å§‹è½¬å½•")
      else:
        acc_audio_data = np.zeros((0,), dtype=np.float32)

        last_transcription_time = time.time()
        last_audio_debug_time = time.time()

        # ç¡®ä¿å¯åŠ¨æ—¶æ›´æ–°UI
        self.update_hud_text("ğŸ”Š æ­£åœ¨ç›‘å¬ç³»ç»ŸéŸ³é¢‘...\næ’­æ”¾éŸ³é¢‘å†…å®¹ä»¥å¼€å§‹è½¬å½•")

      while not self.stop_event.is_set():
        try:
          current_time = time.time()

          # æ¯10ç§’æ‰“å°ä¸€æ¬¡è°ƒè¯•ä¿¡æ¯
          if current_time - last_audio_debug_time > 10:
            print(f"Transcription status: Cumulative audio length {len(acc_audio_data)/self.sample_rate:.2f} seconds")
            last_audio_debug_time = current_time

          # è·å–éŸ³é¢‘æ•°æ®
          try:
            # å®‰å…¨åœ°è·å–é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰æ•°æ®
            audio_data_list = []
            while not self.data_queue.empty():
              try:
                data = self.data_queue.get_nowait()
                audio_data_list.append(data)
              except:
                break

            audio_data = b''.join(audio_data_list)

            if len(audio_data_list) > 0:
              print(f"Detected {len(audio_data_list)} audio data packets")
              print(f"Received audio data: {len(audio_data)} bytes")

              # è½¬æ¢éŸ³é¢‘æ•°æ® - ç°åœ¨ä½¿ç”¨Float32æ ¼å¼
              if args.no_faster_whisper:
                # å¯¹äºæ ‡å‡†whisperï¼Œè½¬æ¢ä¸ºtorch tensor
                audio_np = np.frombuffer(audio_data, dtype=np.float32)  # ç›´æ¥ä½¿ç”¨float32ï¼Œæ— éœ€é™¤æ³•è½¬æ¢
                audio_tensor = torch.from_numpy(audio_np).to(self.compute_device)
                acc_audio_data = torch.cat([acc_audio_data, audio_tensor])
              else:
                # å¯¹äºfaster-whisperï¼Œä½¿ç”¨numpy
                audio_np = np.frombuffer(audio_data, dtype=np.float32)  # ç›´æ¥ä½¿ç”¨float32ï¼Œæ— éœ€é™¤æ³•è½¬æ¢
                acc_audio_data = np.concatenate([acc_audio_data, audio_np])

              print(f"Audio data processed, current cumulative {len(acc_audio_data)/self.sample_rate:.2f} seconds")

          except Exception as e:
            print(f"Error processing audio data: {e}")
            continue

          # æ£€æŸ¥æ˜¯å¦éœ€è¦è½¬å½•
          should_transcribe = False

          # åœ¨å®æ—¶æ¨¡å¼ä¸‹ï¼Œå‡å°æ‰€éœ€çš„æœ€å°éŸ³é¢‘æ•°æ®é‡ä»¥é™ä½å»¶è¿Ÿ
          min_audio_length = 0.2 if realtime_mode else 0.5  # ç§’ - å‡å°‘å»¶è¿Ÿ

          if len(acc_audio_data) >= min_audio_length * self.sample_rate:
            print(f"Audio data sufficient ({len(acc_audio_data)/self.sample_rate:.2f} seconds), starting transcription...")
            should_transcribe = True
          # å¦‚æœéŸ³é¢‘æ•°æ®ä¸å¤Ÿé•¿ï¼Œä½†å·²ç»ç­‰å¾…äº†è¶³å¤Ÿé•¿çš„æ—¶é—´ï¼Œä¹Ÿè¿›è¡Œè½¬å½• - å‡å°‘è¶…æ—¶æ—¶é—´
          elif current_time - last_transcription_time >= 1.0 and len(acc_audio_data) > 0:
            print(f"Timeout reached, processing {len(acc_audio_data)/self.sample_rate:.2f} seconds of audio")
            should_transcribe = True

          if should_transcribe:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é™éŸ³ - è°ƒæ•´é˜ˆå€¼é€‚åº”Float32æ ¼å¼
            if args.no_faster_whisper:
              audio_max = torch.max(torch.abs(acc_audio_data)).item()
            else:
              audio_max = np.max(np.abs(acc_audio_data))

            if audio_max < 0.005:  # é™ä½é˜ˆå€¼ï¼Œé€‚åº”Float32æ ¼å¼çš„éŸ³é¢‘æ•°æ®
              print(f"Audio appears to be silent (max: {audio_max:.6f}), skipping transcription")
              if args.no_faster_whisper:
                acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
              else:
                acc_audio_data = np.array([], dtype=np.float32)
              continue

            print(f"Starting transcription of {len(acc_audio_data)/self.sample_rate:.2f} seconds of audio...")
            print(f"Audio max amplitude: {audio_max:.6f}")
            print("Proceeding with transcription...")

            # ç§»é™¤"æ­£åœ¨è½¬å½•"çŠ¶æ€æ˜¾ç¤ºï¼Œé¿å…é—ªçƒ
            # ç›´æ¥ç­‰å¾…è½¬å½•ç»“æœï¼Œä¿æŒå½“å‰å­—å¹•ç¨³å®šæ˜¾ç¤º

            try:
              # æ‰§è¡Œè½¬å½•
              print("Calling audio_model.transcribe...")

              if args.no_faster_whisper:
                # æ ‡å‡†whisperè½¬å½•
                result = self.audio_model.transcribe(
                  acc_audio_data.cpu().numpy(),
                  language=self.language,
                  task="translate" if args.translate else "transcribe",
                  fp16=not args.no_fp16
                )
                texts = [result['text']]
              else:
                # faster-whisperè½¬å½•
                segments, info = self.audio_model.transcribe(
                  acc_audio_data,
                  language=self.language,
                  task="translate" if args.translate else "transcribe",
                  beam_size=1,
                  best_of=1,
                  temperature=0.0
                )
                texts = [segment.text for segment in segments]

              print("Transcription call completed successfully")
              print("Transcription completed, processing result...")

              if texts and any(text.strip() for text in texts):
                # æœ‰æœ‰æ•ˆçš„è½¬å½•ç»“æœ
                combined_text = ' '.join(texts).strip()
                print(f"Transcription result: '{combined_text}'")

                # ä½¿ç”¨å­—å¹•å†å²ç®¡ç†
                caption_history, current_caption, last_complete_caption, display_text = self.manage_caption_history(
                  [combined_text], caption_history, current_caption, last_complete_caption, max_caption_lines
                )

                if display_text:
                  self.update_hud_text(display_text)

                  # è®¾ç½®ç»“æœæ˜¾ç¤ºçŠ¶æ€ - å»¶é•¿æ˜¾ç¤ºæ—¶é—´
                  last_transcription_result = display_text
                  last_result_display_time = current_time
                  is_showing_result = True
                  print(f"Showing result for {result_display_duration} seconds")
              else:
                print("No speech detected, clearing audio buffer")

            except Exception as e:
              print(f"Error during transcription: {e}")
              import traceback
              traceback.print_exc()

            # æ¸…ç©ºç´¯ç§¯çš„éŸ³é¢‘æ•°æ®
            if args.no_faster_whisper:
              acc_audio_data = torch.zeros((0,), dtype=torch.float32, device=self.compute_device)
            else:
              acc_audio_data = np.array([], dtype=np.float32)

            last_transcription_time = current_time

          # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…é™¤æ˜¾ç¤ºçš„ç»“æœ
          if is_showing_result and current_time - last_result_display_time >= result_display_duration:
            print("Result display time expired, allowing new transcription...")
            is_showing_result = False
            # ä¿æŒå­—å¹•å†å²æ˜¾ç¤ºï¼Œä¸å›åˆ°"ç­‰å¾…"çŠ¶æ€
            # å¦‚æœæœ‰å­—å¹•å†å²ï¼Œç»§ç»­æ˜¾ç¤ºæœ€åçš„å­—å¹•ï¼›å¦‚æœæ²¡æœ‰ï¼Œæ˜¾ç¤ºç­‰å¾…çŠ¶æ€
            if caption_history:
              # é‡æ–°æ˜¾ç¤ºå­—å¹•å†å²ï¼Œä¿æŒç¨³å®šæ˜¾ç¤º
              display_text = '\n'.join(caption_history[-max_caption_lines:])
              self.update_hud_text(display_text)
              print(f"Maintaining caption history display: {len(caption_history)} lines")
            elif not current_caption:
              self.update_hud_text("ğŸ”Š æ­£åœ¨ç›‘å¬ç³»ç»ŸéŸ³é¢‘...\næ’­æ”¾éŸ³é¢‘å†…å®¹ä»¥å¼€å§‹è½¬å½•")

          # çŸ­æš‚ä¼‘çœ ä»¥é¿å…è¿‡åº¦å ç”¨CPU
          sleep(0.05)

        except Exception as e:
          print(f"Error in transcription loop: {e}")
          import traceback
          traceback.print_exc()
          sleep(1)  # å‡ºé”™æ—¶ç¨é•¿çš„ä¼‘çœ 

    except Exception as e:
      print(f"Critical error in transcription thread: {e}")
      import traceback
      traceback.print_exc()
    finally:
      # å§‹ç»ˆæ¸…ç†èµ„æº
      try:
        self.input_provider.stop_record()
      except Exception as e:
        print(f"Error stopping recording: {e}")

    return transcription

def main():
  args = parse_args()

  try:
    # é¦–å…ˆåœ¨ä¸»çº¿ç¨‹åˆ›å»ºQApplication
    app = QApplication([])

    print("Creating system audio caption display window...")

    # åˆ›å»ºHUDçª—å£
    hud_window = HUD(font_size=args.font_size)
    hud_window.show()

    # å¼ºåˆ¶çª—å£æ˜¾ç¤ºåœ¨å‰å°
    hud_window.raise_()
    hud_window.activateWindow()

    print("System audio caption window displayed")

    # å…¨å±€å˜é‡åˆå§‹å†…å®¹
    global text_to_display
    with text_lock:
      text_to_display = "ğŸ”Š ç³»ç»ŸéŸ³é¢‘è½¬å½•å·²å¯åŠ¨\næ’­æ”¾éŸ³é¢‘å†…å®¹ä»¥å¼€å§‹è½¬å½•..."

    print("Creating system audio transcriber...")

    # åˆ›å»ºè½¬å½•å™¨
    transcriber = SystemAudioTranscriber(args)

    print("Starting system audio transcription thread...")

    # å¯åŠ¨è½¬å½•çº¿ç¨‹
    transcriber.start_transcribe_thread()

    # è®¾ç½®ä¿¡å·å¤„ç†
    def handle_signal(sig, frame):
      print("\nReceived interrupt signal, shutting down...")
      transcriber.stop_transcribe_thread()
      app.quit()
    signal.signal(signal.SIGINT, handle_signal)

    print("Starting UI event loop...")

    # è¿è¡Œäº‹ä»¶å¾ªç¯
    app.exec()

    # åœæ­¢è½¬å½•çº¿ç¨‹
    transcriber.stop_transcribe_thread()

  except KeyboardInterrupt:
    print("\nProgram interrupted by user. Exiting...")
  except Exception as e:
    print(f"Critical error in main function: {e}")
    import traceback
    traceback.print_exc()

if __name__ == "__main__":
  main()
